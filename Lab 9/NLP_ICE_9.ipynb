{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzDBnsgaX-K_"
   },
   "source": [
    "# Score DIstribution of Tasks (Rubric)\n",
    "### Task 1: 20%\n",
    "### Task 2: 40%\n",
    "### Task 3: 40 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLOdDUqbVvAK"
   },
   "source": [
    "#Task 1: Run and understand the tutorial\n",
    "# Smoothing techniques commonly used in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcZBiCxzVvAQ"
   },
   "source": [
    "In this notebook, I will introduce several smoothing techniques commonly used in NLP or machine learning algorithms. They are:\n",
    "- Laplacian (add-one) Smoothing\n",
    "- Lidstone (add-k) Smoothing\n",
    "- Absolute Discounting\n",
    "- Katz Backoff\n",
    "- Kneser-Ney Smoothing\n",
    "- Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKH9dekAVvAR"
   },
   "source": [
    "Before starting to implement these smoothing methods, we first need to build a *N*-gram language model, and then applying different smoothing methods to this language model, evaluating the results between these smoothing techniques. In this notebook, I will build a bigram language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuM6Z1EQVvAS"
   },
   "source": [
    "Now, let's define some notations used in the following programs. In this notebook, **token** is the number of words in a document or a sentence, **vocab** is the number of different type of words in a document or sentence. For example, in the following sentence, there are 10 tokens and 8 vocabs (because \"I\" and \"like\" occur two times).  \n",
    "\"I like natural language processing and I like machine learning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JJafQ56VvAS"
   },
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dCBAg5QWVvAT"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from numpy.random import choice \n",
    "from tqdm import tqdm\n",
    "\n",
    "class Bigram():\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "        self.context = defaultdict(Counter)\n",
    "        self.start_count = 0\n",
    "        self.token_count = 0\n",
    "        self.vocab_count = 0\n",
    "    \n",
    "    def convert_sentence(self, sentence):\n",
    "        return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
    "    \n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts\n",
    "        for sentence in sentences:\n",
    "            sentence = self.convert_sentence(sentence)\n",
    "            for word in sentence[1:]:  # from 1, because we don't need the <s> token\n",
    "                self.unigram_counts[word] += 1\n",
    "            self.start_count += 1\n",
    "            \n",
    "        # collect bigram counts\n",
    "        for sentence in sentences:\n",
    "            sentence = self.convert_sentence(sentence)\n",
    "            bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "            for bigram in bigram_list:\n",
    "                self.bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "                self.context[bigram[1]][bigram[0]] += 1\n",
    "        self.token_count = sum(self.unigram_counts.values())\n",
    "        self.vocab_count = len(self.unigram_counts.keys())\n",
    "        \n",
    "    def generate_sentence(self):\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        while current_word != \"</s>\":\n",
    "            prev_word = current_word\n",
    "            prev_word_counts = self.bigram_counts[prev_word]\n",
    "            # obtain bigram probability distribution given the previous word\n",
    "            bigram_probs = []\n",
    "            total_counts = float(sum(prev_word_counts.values()))\n",
    "            for word in prev_word_counts:\n",
    "                bigram_probs.append(prev_word_counts[word] / total_counts)\n",
    "            # sample the next word\n",
    "            current_word = choice(list(prev_word_counts.keys()), p=bigram_probs)\n",
    "            sentence.append(current_word)\n",
    "            \n",
    "        sentence = \" \".join(sentence[1:-1])\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVd1d-g-VvAV"
   },
   "source": [
    "Now we have finished building our bigram language model without any smoothing. Let's try to generate some sentences using the Penn Treebank corpora as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcnMz0qZVvAW",
    "outputId": "47eaa89f-becf-469f-85b9-1f60e242407a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "you have to my father seemed nothing has settled her fear .\n",
      "Sentence 2\n",
      "russ wiped some cases .\n",
      "Sentence 3\n",
      "by a more familiar with his guns and move .\n",
      "Sentence 4\n",
      "at the youngest clerk was the difference is astarte , aimed wholly in the mount vernon reports .\n",
      "Sentence 5\n",
      "figures i've never be memorized .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "# nltk.download('brown')\n",
    "\n",
    "bigram = Bigram()\n",
    "bigram.get_counts(brown.sents())\n",
    "for i in range(1,6):\n",
    "    print(\"Sentence %d\" % i)\n",
    "    print(bigram.generate_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHvwaX-TVvAX"
   },
   "source": [
    "The output for our sample sentence looks reasonable, Now, let's use perplexity to evaluate different smoothing techniques at the level of the corpus. For this, we'll divide Brown corpus up randomly into a training set and a test set based on an 80/20 split. The perplexity can be calculated as follow:\n",
    "\n",
    "$PP(W) = \\sqrt[m]{\\frac{1}{P(W)}}$\n",
    "\n",
    "$\\log{PP(W)} = -\\frac{1}{m} \\log{P(W)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MLm-OjAYVvAY"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from random import shuffle\n",
    "\n",
    "def split_train_test():\n",
    "    sents = list(brown.sents())\n",
    "    shuffle(sents)\n",
    "    cutoff = int(0.8*len(sents))\n",
    "    training_set = sents[:cutoff]\n",
    "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
    "    return training_set, test_set\n",
    "\n",
    "def calculate_perplexity(sentences, bigram, smoothing_function, parameter):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in tqdm(sentences):\n",
    "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
    "        total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
    "    return math.exp(-total_log_prob / test_token_count)\n",
    "\n",
    "training_set, test_set = split_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REpNJQxvVvAZ"
   },
   "source": [
    "Until Now, we can evaluate the quality of different smoothing methods via calculating perplexity of test set. Now let's start to learn these smoothing techniques. For better understanding, here we use a sample example to explain these smoothing methods. Supposing we have 7 vocabs and their counts are as follows: **(Note this is a simplified example which is more like a unigram model)**\n",
    "\n",
    "vocabs | counts | unsmoothed probability\n",
    ":-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | \n",
    "offense | 5 | 0.25 | \n",
    "damage | 4 | 0.2 | \n",
    "deficiencies | 2 | 0.1 | \n",
    "outbreak | 1 | 0.05 | \n",
    "infirmity | 0 | 0 | \n",
    "cephalopods | 0 | 0 | \n",
    "**total** | **20** | **1.0** | \n",
    "\n",
    "A bigram model without any smoothing can be formulated as follow: \n",
    "$$ P(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i})}{C(w_{i-1})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWXbd7ALVvAZ"
   },
   "source": [
    "## Laplacian (add-one) Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4QZAlehVvAa"
   },
   "source": [
    "Laplacian (add-one) smoothing: \n",
    "\n",
    "$$ P_{add-1}(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i}) + 1}{C(w_{i-1}) + |V|}$$\n",
    "\n",
    "**Core idea**: Pretend that we have seen each vocab at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmYql3ThVvAa"
   },
   "source": [
    "vocabs | counts | unsmoothed probability | laplacian (add-one) smoothing\n",
    ":-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8+1)/(20+7)= 0.333\n",
    "offense | 5 | 0.25 | (5+1)/(20+7)= 0.222\n",
    "damage | 4 | 0.2 | (4+1)/(20+7)= 0.186\n",
    "deficiencies | 2 | 0.1 | (2+1)/(20+7)= 0.111\n",
    "outbreak | 1 | 0.05 | (1+1)/(20+7)= 0.074\n",
    "infirmity | 0 | 0 | (0+1)/(20+7)= 0.037\n",
    "cephalopods | 0 | 0 | (0+1)/(20+7)= 0.037\n",
    "**total** | **20** | **1.0** | **1.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sjaQQ1oVvAa",
    "outputId": "a56015dd-a450-4b35-f5f3-dae25ec4ad74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 11468/11468 [00:00<00:00, 27625.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3486.254398478114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def laplacian_smoothing(sentence, bigram, parameter):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + 1\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + len(bigram.unigram_counts)\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_laplacian_smoothing = Bigram()\n",
    "bigram_laplacian_smoothing.get_counts(training_set)\n",
    "plex_laplacian_smoothing = calculate_perplexity(test_set, bigram_laplacian_smoothing, laplacian_smoothing, None)\n",
    "print(plex_laplacian_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4756/3878765992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_laplacian_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4756/3878765992.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(sentence, bigram)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mbigram_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4756/1820961042.py\u001b[0m in \u001b[0;36mconvert_sentence\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"<s>\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"</s>\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4756/1820961042.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"<s>\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"</s>\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "text = [['Meme-based', 'cryptocurrency', 'shiba', 'inu', 'soared', 'more', 'than', '45%', 'over', 'the', 'past', '24', 'hours,', 'muscling', \n",
    "         'into', 'the', 'top-10', 'largest', 'digital', 'tokens', 'by', 'market', 'capitalization.'], \n",
    "        ['Shiba', 'inu', 'is', 'a', 'spinoff', 'of', 'dogecoin,', 'itself', 'born', 'as', 'a', 'satire', 'of', 'a', 'cryptocurrency', 'frenzy', \n",
    "         'in', '2013,', 'and', 'has', 'barely', 'any', 'practical', 'use.']]\n",
    "bigram_laplacian_smoothing = Bigram()\n",
    "bigram_laplacian_smoothing.get_counts(text)\n",
    "\n",
    "def test(sentence, bigram):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + 1\n",
    "        print(f'first sm_bigram_counts: {sm_bigram_counts}')\n",
    "        if prev_word == \"<s>\": \n",
    "            sm_unigram_counts = bigram.start_count\n",
    "        else: \n",
    "            sm_unigram_counts = bigram.unigram_counts[prev_word] + len(bigram.unigram_counts)\n",
    "            print(f'sm_unigram_counts {sm_unigram_counts}')\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "        print(f'prob: {prob}')\n",
    "    return prob\n",
    "\n",
    "test(text, bigram_laplacian_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram counts\n",
      "defaultdict(<class 'collections.Counter'>, {'<s>': Counter({'meme-based': 1, 'shiba': 1}), 'meme-based': Counter({'cryptocurrency': 1}), 'cryptocurrency': Counter({'shiba': 1, 'frenzy': 1}), 'shiba': Counter({'inu': 2}), 'inu': Counter({'soared': 1, 'is': 1}), 'soared': Counter({'more': 1}), 'more': Counter({'than': 1}), 'than': Counter({'45%': 1}), '45%': Counter({'over': 1}), 'over': Counter({'the': 1}), 'the': Counter({'past': 1, 'top-10': 1}), 'past': Counter({'24': 1}), '24': Counter({'hours,': 1}), 'hours,': Counter({'muscling': 1}), 'muscling': Counter({'into': 1}), 'into': Counter({'the': 1}), 'top-10': Counter({'largest': 1}), 'largest': Counter({'digital': 1}), 'digital': Counter({'tokens': 1}), 'tokens': Counter({'by': 1}), 'by': Counter({'market': 1}), 'market': Counter({'capitalization.': 1}), 'capitalization.': Counter({'</s>': 1}), 'is': Counter({'a': 1}), 'a': Counter({'spinoff': 1, 'satire': 1, 'cryptocurrency': 1}), 'spinoff': Counter({'of': 1}), 'of': Counter({'dogecoin,': 1, 'a': 1}), 'dogecoin,': Counter({'itself': 1}), 'itself': Counter({'born': 1}), 'born': Counter({'as': 1}), 'as': Counter({'a': 1}), 'satire': Counter({'of': 1}), 'frenzy': Counter({'in': 1}), 'in': Counter({'2013,': 1}), '2013,': Counter({'and': 1}), 'and': Counter({'has': 1}), 'has': Counter({'barely': 1}), 'barely': Counter({'any': 1}), 'any': Counter({'practical': 1}), 'practical': Counter({'use.': 1}), 'use.': Counter({'</s>': 1})})\n",
      "\n",
      "Unigram counts\n",
      "Counter({'a': 3, 'cryptocurrency': 2, 'shiba': 2, 'inu': 2, 'the': 2, '</s>': 2, 'of': 2, 'meme-based': 1, 'soared': 1, 'more': 1, 'than': 1, '45%': 1, 'over': 1, 'past': 1, '24': 1, 'hours,': 1, 'muscling': 1, 'into': 1, 'top-10': 1, 'largest': 1, 'digital': 1, 'tokens': 1, 'by': 1, 'market': 1, 'capitalization.': 1, 'is': 1, 'spinoff': 1, 'dogecoin,': 1, 'itself': 1, 'born': 1, 'as': 1, 'satire': 1, 'frenzy': 1, 'in': 1, '2013,': 1, 'and': 1, 'has': 1, 'barely': 1, 'any': 1, 'practical': 1, 'use.': 1})\n",
      "\n",
      "Context\n",
      "defaultdict(<class 'collections.Counter'>, {'meme-based': Counter({'<s>': 1}), 'cryptocurrency': Counter({'meme-based': 1, 'a': 1}), 'shiba': Counter({'cryptocurrency': 1, '<s>': 1}), 'inu': Counter({'shiba': 2}), 'soared': Counter({'inu': 1}), 'more': Counter({'soared': 1}), 'than': Counter({'more': 1}), '45%': Counter({'than': 1}), 'over': Counter({'45%': 1}), 'the': Counter({'over': 1, 'into': 1}), 'past': Counter({'the': 1}), '24': Counter({'past': 1}), 'hours,': Counter({'24': 1}), 'muscling': Counter({'hours,': 1}), 'into': Counter({'muscling': 1}), 'top-10': Counter({'the': 1}), 'largest': Counter({'top-10': 1}), 'digital': Counter({'largest': 1}), 'tokens': Counter({'digital': 1}), 'by': Counter({'tokens': 1}), 'market': Counter({'by': 1}), 'capitalization.': Counter({'market': 1}), '</s>': Counter({'capitalization.': 1, 'use.': 1}), 'is': Counter({'inu': 1}), 'a': Counter({'is': 1, 'as': 1, 'of': 1}), 'spinoff': Counter({'a': 1}), 'of': Counter({'spinoff': 1, 'satire': 1}), 'dogecoin,': Counter({'of': 1}), 'itself': Counter({'dogecoin,': 1}), 'born': Counter({'itself': 1}), 'as': Counter({'born': 1}), 'satire': Counter({'a': 1}), 'frenzy': Counter({'cryptocurrency': 1}), 'in': Counter({'frenzy': 1}), '2013,': Counter({'in': 1}), 'and': Counter({'2013,': 1}), 'has': Counter({'and': 1}), 'barely': Counter({'has': 1}), 'any': Counter({'barely': 1}), 'practical': Counter({'any': 1}), 'use.': Counter({'practical': 1})})\n",
      "\n",
      "Start count\n",
      "2\n",
      "\n",
      "Token count\n",
      "49\n",
      "\n",
      "Vocab count\n",
      "41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_laplacian_smoothing = Bigram()\n",
    "unigram_laplacian_smoothing.get_counts(text)\n",
    "\n",
    "print('Bigram counts')\n",
    "print(unigram_laplacian_smoothing.bigram_counts)\n",
    "print()\n",
    "print('Unigram counts')\n",
    "print(unigram_laplacian_smoothing.unigram_counts)\n",
    "print()\n",
    "print('Context')\n",
    "print(unigram_laplacian_smoothing.context)\n",
    "print()\n",
    "print('Start count')\n",
    "print(unigram_laplacian_smoothing.start_count)\n",
    "print()\n",
    "print('Token count')\n",
    "print(unigram_laplacian_smoothing.token_count)\n",
    "print()\n",
    "print('Vocab count')\n",
    "print(unigram_laplacian_smoothing.vocab_count)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unilaplacian_smoothing(sentence, bigram, parameter):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    unigram_list = sentence\n",
    "    prob = 0\n",
    "    for word in unigram_list:\n",
    "        sm_unigram_counts = bigram.unigram_counts[word] + 1\n",
    "        if \n",
    "            word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: \n",
    "#             sm_unigram_counts = bigram.unigram_counts[word] + len(bigram.unigram_counts)\n",
    "            \n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdiAxWt_VvAb"
   },
   "source": [
    "## Lidstone (add-k) Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9sGT8GHVvAc"
   },
   "source": [
    "Lidstone (add-k) smoothing: \n",
    "\n",
    "$$ P_{add-k}(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i}) + k}{C(w_{i-1}) + k|V|}$$\n",
    "\n",
    "**Core idea**: Sometimes adding one is too much, instead, we add k (usually k < 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpeR0W3WVvAd"
   },
   "source": [
    "vocabs | counts | unsmoothed probability | lidstone (add-k) smoothing (k=0.05)\n",
    ":-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8+0.5)/(20+7*0.5)= 0.363\n",
    "offense | 5 | 0.25 | (5+0.5)/(20+7*0.5)= 0.234\n",
    "damage | 4 | 0.2 | (4+0.5)/(20+7*0.5)= 0.191\n",
    "deficiencies | 2 | 0.1 | (2+0.5)/(20+7*0.5)= 0.106\n",
    "outbreak | 1 | 0.05 | (1+0.5)/(20+7*0.5)= 0.064\n",
    "infirmity | 0 | 0 | (0+0.5)/(20+7*0.5)= 0.021\n",
    "cephalopods | 0 | 0 | (0+0.5)/(20+7*0.5)= 0.021\n",
    "**total** | **20** | **1.0** | **1.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8EJAp6IVvAd",
    "outputId": "226fdabc-ac2f-485b-be14-a9a0716ebf53"
   },
   "outputs": [],
   "source": [
    "def lidstone_smoothing(sentence, bigram, k):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + k\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + k*len(bigram.unigram_counts)\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_lidstone_smoothing = Bigram()\n",
    "bigram_lidstone_smoothing.get_counts(training_set)\n",
    "plex_lidstone_smoothing = calculate_perplexity(test_set, bigram_lidstone_smoothing, lidstone_smoothing, 0.05)\n",
    "print(plex_lidstone_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_Vgg4PTVvAe"
   },
   "source": [
    "## Absolute Discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7SHhLYTVvAe"
   },
   "source": [
    "Absolute discounting:\n",
    "\n",
    "$$ P_{absolute-discounting}(w_{i}|w_{i-1})=\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
    "\\alpha(w_{i-1}) / \\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}, otherwise\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "**Core idea**: 'Borrows' a fixed probability mass from observed n-gram counts and redistributes it to unseen n-grams.\n",
    "\n",
    "$\\alpha(w_{i-1})$ is the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.\n",
    "\n",
    "$\\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}$ is the count of $C(w_{i-1}, w_{j})=0$, here it is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ7zmx8SVvAe"
   },
   "source": [
    "vocabs | counts | unsmoothed probability | absolute discounting (d=0.1) | effective counts\n",
    ":-: | :-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
    "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
    "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
    "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
    "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
    "infirmity | 0 | 0 | (0+0.5)/20/2=0.0125 | 0.25\n",
    "cephalopods | 0 | 0 | (0+0.5)/20/2=0.0125 | 0.25\n",
    "**total** | **20** | **1.0** | **1.0** | **20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTNmceDBVvAe",
    "outputId": "c9ee1df4-fa34-4304-c25a-6740a2980f1c"
   },
   "outputs": [],
   "source": [
    "def absolute_discounting(sentence, bigram, d):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
    "        if sm_unigram_counts == 0: \n",
    "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
    "            continue\n",
    "        if sm_bigram_counts != 0: \n",
    "            sm_bigram_counts = sm_bigram_counts - d\n",
    "        else: \n",
    "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
    "            # count how many vocabs do not appear after pre_word\n",
    "            prev_word_discounting = bigram.vocab_count - alpha_prev_word\n",
    "            sm_bigram_counts = alpha_prev_word * d / prev_word_discounting\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_absolute_discounting = Bigram()\n",
    "bigram_absolute_discounting.get_counts(training_set)\n",
    "plex_absolute_discounting = calculate_perplexity(test_set, bigram_absolute_discounting, absolute_discounting, 0.1)\n",
    "print(plex_absolute_discounting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo9RyzXoVvAf"
   },
   "source": [
    "## Katz Backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnAKK43FVvAg"
   },
   "source": [
    "Katz Backoff:\n",
    "\n",
    "$$ P_{backoff}(w_{i}|w_{i-1})=\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
    "\\alpha(w_{i-1}) \\times \\frac{P(w_{j})}{\\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}{P(w_{j})}}, otherwise\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "**Core idea**: Absolute discounting redistributes the probability mass **equally** for all unseen n-grams while Backoff redistributes the mass based on a lower order model (e.g. unigram).\n",
    "\n",
    "$\\alpha(w_{i-1})$ is also the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.\n",
    "\n",
    "$P(w_{i})$ is the unigram probability for $w_{i}$. Suppose here $P(infirmity) = 0.002$, $P(cephalopods) = 0.008$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPIQCWc7VvAg"
   },
   "source": [
    "vocabs | counts | unsmoothed probability | backoff | effective counts\n",
    ":-: | :-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
    "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
    "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
    "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
    "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
    "infirmity | 0 | 0 | (0+0.5)/20*0.002/(0.002+0.008)=0.0005 | 0.1\n",
    "cephalopods | 0 | 0 | (0+0.5)/20*0.008/(0.002+0.008)=0.02 | 0.4\n",
    "**total** | **20** | **1.0** | **1.0** | **20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5sRWCd9rVvAh",
    "outputId": "e89fcdfe-72c9-4c8f-bc1d-eda79943475e"
   },
   "outputs": [],
   "source": [
    "def backoff(sentence, bigram, d):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
    "        if sm_unigram_counts == 0: \n",
    "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
    "            continue\n",
    "        if sm_bigram_counts != 0: \n",
    "            sm_bigram_counts = sm_bigram_counts - d\n",
    "        else: \n",
    "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
    "            # sum unigram counts of word j which do not appear after pre_word\n",
    "            unseen_unigram_sum = 0\n",
    "            for vocab in bigram.unigram_counts.keys():\n",
    "                if vocab not in bigram.bigram_counts[prev_word].keys():\n",
    "                    unseen_unigram_sum += bigram.unigram_counts[vocab]\n",
    "            unseen_unigram = bigram.unigram_counts[word] / unseen_unigram_sum\n",
    "            if unseen_unigram == 0: unseen_unigram = 1 / float(bigram.vocab_count - alpha_prev_word)\n",
    "            sm_bigram_counts = alpha_prev_word * d * unseen_unigram\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_backoff = Bigram()\n",
    "bigram_backoff.get_counts(training_set)\n",
    "plex_backoff = calculate_perplexity(test_set, bigram_backoff, backoff, 0.1)\n",
    "print(plex_backoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wry0JS37VvAi"
   },
   "source": [
    "## Kneser-Ney Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qLbTWhPVvAi"
   },
   "source": [
    "Kneser-Ney Smoothing:\n",
    "\n",
    "$$ P_{kneser-ney-smoothing}(w_{i}|w_{i-1})=\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
    "\\alpha(w_{i-1})P_{cont}(w_{i}), otherwise\n",
    "\\end{aligned}\n",
    "\\right.\\\\\n",
    "where \\quad\n",
    "P_{cont}(w_{i}) = \\frac{|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|}{{\\sum_{w_{i}}{|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|}}}\n",
    "$$\n",
    "\n",
    "**Core idea**: Redistribute probability mass based on how many number of different contexts word w has appeared in.\n",
    "\n",
    "$\\alpha(w_{i-1})$ is also the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.  \n",
    "Suppose we have the following phrases in the corpus: {A infirmity, B infirmity, C infirmity, D infirmity, A cephalopods}, then  \n",
    "$|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|$ for $w_{i}$ = infirmity is 4, $P_{cont}(w_{i}=infirmity)$ = 4/(4+1)= 0.8.  \n",
    "$|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|$ for $w_{i}$ = cephalopods is 1, $P_{cont}(w_{i}=cephalopods)$ = 1/(4+1)= 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq7TxWOSVvAi"
   },
   "source": [
    "vocabs | counts | unsmoothed probability | kneser-ney smoothing | effective counts\n",
    ":-: | :-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
    "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
    "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
    "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
    "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
    "infirmity | 0 | 0 | (0+0.5)/20*4/(4+1)=0.02 | 0.4\n",
    "cephalopods | 0 | 0 | (0+0.5)/20*1/(4+1)=0.005 | 0.1\n",
    "**total** | **20** | **1.0** | **1.0** | **20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ji5SJpXZVvAi"
   },
   "outputs": [],
   "source": [
    "def kneser_ney_smoothing(sentence, bigram, d):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
    "        if sm_unigram_counts == 0: \n",
    "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
    "            continue\n",
    "        if sm_bigram_counts != 0: \n",
    "            sm_bigram_counts = sm_bigram_counts - d\n",
    "        else: \n",
    "            # statistic how many tokens not occureed after pre_word\n",
    "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
    "            \n",
    "            context_sum = 0\n",
    "            for vocab in bigram.unigram_counts.keys():\n",
    "                if vocab not in bigram.bigram_counts[prev_word].keys():\n",
    "                    context_sum += len(bigram.context[vocab].keys())\n",
    "            p_cont = len(bigram.context[word].keys()) / context_sum\n",
    "            if p_cont == 0: p_cont = 1 / float(bigram.vocab_count - alpha_prev_word)\n",
    "            sm_bigram_counts = alpha_prev_word * d * p_cont\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_kneser_ney_smoothing = Bigram()\n",
    "bigram_kneser_ney_smoothing.get_counts(training_set)\n",
    "plex_kneser_ney_smoothing = calculate_perplexity(test_set, bigram_kneser_ney_smoothing, kneser_ney_smoothing, 0.1)\n",
    "print(plex_kneser_ney_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rifVT0J1VvAi"
   },
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxovDxSNVvAj"
   },
   "source": [
    "Interpolation:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "P_{interpolation}(w_{i}|w_{i-1}, w_{i-2})&=\\lambda_{3}P_{3}(w_{i}|w_{i-1}, w_{i-2}) \\\\\n",
    "&+\\lambda_{2}P_{2}(w_{i}|w_{i-1})\\\\\n",
    "&+\\lambda_{1}P_{1}(w_{i})\\\\\n",
    "where \\quad\n",
    "\\sum_{i}{\\lambda_{i}} = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Core idea**: Combine different order n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zm2j-acPVvAj"
   },
   "outputs": [],
   "source": [
    "def interpolation(sentence, bigram, lambdas):\n",
    "    bigram_lambda = lambdas[0]\n",
    "    unigram_lambda = lambdas[1]\n",
    "    zerogram_lambda = 1 - lambdas[0] - lambdas[1]\n",
    "    \n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "    \n",
    "    for prev_word, word in bigram_list:\n",
    "        # bigram probability\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
    "        if sm_bigram_counts == 0: interp_bigram_counts = 0\n",
    "        else:\n",
    "            if prev_word == \"<s>\": u_counts = bigram.start_count\n",
    "            else: u_counts = bigram.unigram_counts[prev_word]\n",
    "            interp_bigram_counts = sm_bigram_counts / float(u_counts) * bigram_lambda\n",
    "\n",
    "        # unigram probability\n",
    "        interp_unigram_counts = (bigram.unigram_counts[word] / bigram.token_count) * unigram_lambda\n",
    "\n",
    "        # \"zerogram\" probability: this is to account for out-of-vocabulary words, this is just 1 / |V|\n",
    "        vocab_size = len(bigram.unigram_counts)\n",
    "        interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
    "    \n",
    "        prob += math.log(interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_interpolation = Bigram()\n",
    "bigram_interpolation.get_counts(training_set)\n",
    "plex_interpolation = calculate_perplexity(test_set, bigram_interpolation, interpolation, (0.8, 0.19))\n",
    "print(plex_interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08DqeLzeVvAj"
   },
   "source": [
    "Now we have finished our work, the following table shows the perplexity of different smoothing methods. We can learn that different smoothing techniques may greatly affect the quality of language models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv-L-E4XVvAj"
   },
   "source": [
    "smoothing techniques | perpleity on Brown test corpus\n",
    ":-: | :-: \n",
    "Laplacian (add-one) Smoothing | 3508\n",
    "Lidstone (add-k) Smoothing | 1188\n",
    "Absolute Discounting | 1013\n",
    "Katz Backoff | 588\n",
    "Kneser-Ney Smoothing | 569\n",
    "Interpolation | 436"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLU8Zyf2VvAk"
   },
   "source": [
    "The above implementations may not be optimal according to efficiency and memory, but it shows how different smoothing techniques work in a language model intuitively, so it may be a good tutorial for some beginners of NLP. If there are some mistakes in the code, welcome to point it out and I will correct it as soon as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgU_zdAKXjqW"
   },
   "source": [
    "# Task 2\n",
    "## Implement the above smoothing techinques for Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sjaQQ1oVvAa",
    "outputId": "a56015dd-a450-4b35-f5f3-dae25ec4ad74"
   },
   "outputs": [],
   "source": [
    "def unigram_laplacian_smoothing(sentence, bigram, parameter):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    unigram_list = sentence\n",
    "    prob = 0\n",
    "    for prev_word, word in unigram_list:\n",
    "        sm_unigram_counts = bigram.unigram_counts[prev_word][word] + 1\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + len(bigram.unigram_counts)\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_laplacian_smoothing = Bigram()\n",
    "bigram_laplacian_smoothing.get_counts(training_set)\n",
    "plex_laplacian_smoothing = calculate_perplexity(test_set, bigram_laplacian_smoothing, laplacian_smoothing, None)\n",
    "print(plex_laplacian_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B4ROfLjXr7B"
   },
   "source": [
    "# Task 3\n",
    "## Implement the above smoothing techinques for Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlLs7a6MWVEm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_ICE-9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
