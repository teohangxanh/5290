{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICE7 Nghia Dang.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waiZ70w4Omp-",
        "outputId": "54428979-625c-4fb5-adcb-94f1dc70c9b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ELMaKQGPL1N",
        "outputId": "8a8e6102-c1e5-41ad-d9ae-5125bb963688"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install torch>=1.3.1\n",
        "!pip install torchtext==0.4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.4\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.9.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4) (3.7.4.3)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tidDzfNnOpV9",
        "outputId": "413aa2cb-650e-4656-cb47-28e2058d2f72"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "#from torchtext.legacy.datasets import text_classification\n",
        "from torchtext.datasets import text_classification\n",
        "\n",
        "import os\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', vocab=None)\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ag_news_csv.tar.gz: 100%|██████████| 11.8M/11.8M [00:00<00:00, 107MB/s]\n",
            "120000lines [00:09, 12515.93lines/s]\n",
            "120000lines [00:19, 6019.88lines/s]\n",
            "7600lines [00:01, 6375.64lines/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvZbEMEEO4US"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1kNUFr_ZuHz"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7o6i3goZitO"
      },
      "source": [
        "max_seq_len = 50\n",
        "TEXT = data.Field(tokenize=\"spacy\", batch_first=True, include_lengths=True, fix_length=max_seq_len)\n",
        "LABEL = data.LabelField(dtype=torch.float, batch_first=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urjXjeBTZe5d"
      },
      "source": [
        "fields = [('label', LABEL), (None, None), ('text',TEXT)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbU3yoPnaJPb",
        "outputId": "17d3e635-2425-4625-f2ec-f76f3fb99382"
      },
      "source": [
        "%ls drive/MyDrive/UNT/AG_news"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'drive/MyDrive/UNT/AG_news': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JPQukhmZhR5"
      },
      "source": [
        "training_data=data.TabularDataset(path = 'drive/MyDrive/train.csv',format = 'csv',fields = fields,skip_header = True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS0sg4RRbdNB",
        "outputId": "5850442a-9d89-497d-ffe0-88610b5969ed"
      },
      "source": [
        "print(vars(training_data.examples[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': '3', 'text': ['Reuters', '-', 'Private', 'investment', 'firm', 'Carlyle', 'Group,\\\\which', 'has', 'a', 'reputation', 'for', 'making', 'well', '-', 'timed', 'and', 'occasionally\\\\controversial', 'plays', 'in', 'the', 'defense', 'industry', ',', 'has', 'quietly', 'placed\\\\its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market', '.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uP8ND6Pbeoc"
      },
      "source": [
        "train_data, valid_data = training_data.split(split_ratio=0.1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNgRJDduZpE6",
        "outputId": "ab6b753c-65be-4c86-c9b3-4d4242ecf11d"
      },
      "source": [
        "#initialize glove embeddings\n",
        "TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.300d\")  \n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "# print(TEXT.vocab.stoi)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:45<00:00, 8789.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of TEXT vocabulary: 2269\n",
            "Size of LABEL vocabulary: 4\n",
            "[('the', 1674), (',', 1365), ('.', 1252), ('-', 1059), ('a', 970), ('of', 906), ('to', 869), ('in', 757), ('and', 616), (' ', 545)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a5Sylp9bQG4"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data), batch_size=batch_size,\n",
        "                                                           sort_key=lambda x: len(x.text),\n",
        "                                                           sort_within_batch=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oeIdcP6bj7R"
      },
      "source": [
        "# Create neural network representation\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class CNNTextClassification(nn.Module):\n",
        "    def __init__(self, vocabulary_size, embedding_size, max_seq_len, out_channels,\n",
        "                 kernel_heights, dropout, num_class):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_heights = kernel_heights\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
        "        \n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=self.embedding_size, out_channels=self.out_channels,\n",
        "                               kernel_size=self.kernel_heights[0]),\n",
        "                                   nn.ReLU(),\n",
        "                                  nn.MaxPool1d(self.max_seq_len - self.kernel_heights[0]+1))\n",
        "        \n",
        "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=self.embedding_size, out_channels=self.out_channels,\n",
        "                               kernel_size=self.kernel_heights[1]),\n",
        "                                   nn.ReLU(),\n",
        "                                  nn.MaxPool1d(self.max_seq_len - self.kernel_heights[1]+1))\n",
        "        \n",
        "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=self.embedding_size, out_channels=self.out_channels,\n",
        "                               kernel_size=self.kernel_heights[2]),\n",
        "                                   nn.ReLU(),\n",
        "                                  nn.MaxPool1d(self.max_seq_len - self.kernel_heights[2]+1))\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(len(self.kernel_heights) * out_channels, num_class)\n",
        "        \n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        emb = self.embedding(text).permute(0, 2, 1)\n",
        "        \n",
        "        conv_out1 = self.conv1(emb).squeeze(2)\n",
        "        conv_out2 = self.conv2(emb).squeeze(2)\n",
        "        conv_out3 = self.conv3(emb).squeeze(2)\n",
        "        \n",
        "        all_out = torch.cat((conv_out1, conv_out2, conv_out3), 1)\n",
        "        final_feature_map = self.dropout(all_out)\n",
        "        \n",
        "        final_out = self.fc(final_feature_map)\n",
        "        \n",
        "        return self.softmax(final_out)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doRqdTJJvVlM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(RNN, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.rnn = nn.RNN(embedding_length, hidden_size, num_layers=2, bidirectional=True)\n",
        "\t\tself.label = nn.Linear(4*hidden_size, output_size)\n",
        "\t\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\t\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the final_hidden_state of RNN.\n",
        "\t\tlogits.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda()) # 4 = num_layers*num_directions\n",
        "\t\telse:\n",
        "\t\t\th_0 =  Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, h_n = self.rnn(input, h_0)\n",
        "\t\t# h_n.size() = (4, batch_size, hidden_size)\n",
        "\t\th_n = h_n.permute(1, 0, 2) # h_n.size() = (batch_size, 4, hidden_size)\n",
        "\t\th_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
        "\t\t# h_n.size() = (batch_size, 4*hidden_size)\n",
        "\t\tlogits = self.label(h_n) # logits.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\treturn logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPTHqi20vqVs"
      },
      "source": [
        "# Use LSTM for the above scenario\n",
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(LSTMClassifier, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t\n",
        "\tdef forward(self, input_sentence, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
        "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
        "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\treturn final_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0YGUrMWvwmE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "class AttentionModel(torch.nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(AttentionModel, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t#self.attn_fc_layer = nn.Linear()\n",
        "\t\t\n",
        "\tdef attention_net(self, lstm_output, final_state):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tNow we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
        "\t\tbetween each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
        "\t\t\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tlstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
        "\t\tfinal_state : Final time-step hidden state (h_n) of the LSTM\n",
        "\t\t\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tReturns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
        "\t\t\t\t  new hidden state.\n",
        "\t\t\t\t  \n",
        "\t\tTensor Size :\n",
        "\t\t\t\t\thidden.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\tattn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tsoft_attn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tnew_hidden_state.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\t  \n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\thidden = final_state.squeeze(0)\n",
        "\t\tattn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "\t\tsoft_attn_weights = F.softmax(attn_weights, 1)\n",
        "\t\tnew_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "\t\t\n",
        "\t\treturn new_hidden_state\n",
        "\t\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\t\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \n",
        "\t\toutput = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
        "\t\t\n",
        "\t\tattn_output = self.attention_net(output, final_hidden_state)\n",
        "\t\tlogits = self.label(attn_output)\n",
        "\t\t\n",
        "\t\treturn logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "873O09wGv5fw"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(SelfAttention, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\tself.weights = weights\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.dropout = 0.8\n",
        "\t\tself.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
        "\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
        "\t\tself.W_s1 = nn.Linear(2*hidden_size, 350)\n",
        "\t\tself.W_s2 = nn.Linear(350, 30)\n",
        "\t\tself.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
        "\t\tself.label = nn.Linear(2000, output_size)\n",
        "\n",
        "\tdef attention_net(self, lstm_output):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
        "\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n",
        "\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n",
        "\t\tconnected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n",
        "\t\tpos & neg.\n",
        "\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\n",
        "\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
        "\t\t---------\n",
        "\n",
        "\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
        "\t\t\t\t  attention to different parts of the input sentence.\n",
        "\n",
        "\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tattn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
        "\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "\n",
        "\t\treturn attn_weight_matrix\n",
        "\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "\t\toutput, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
        "\t\toutput = output.permute(1, 0, 2)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t# h_n.size() = (1, batch_size, hidden_size)\n",
        "\t\t# c_n.size() = (1, batch_size, hidden_size)\n",
        "\t\tattn_weight_matrix = self.attention_net(output)\n",
        "\t\t# attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\thidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
        "\t\t# hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
        "\t\t# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
        "\t\tfc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
        "\t\tlogits = self.label(fc_out)\n",
        "\t\t# logits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\treturn logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBSb4jBzwFMx"
      },
      "source": [
        "def training(model, iterator, optimizer, criterion):\n",
        "    training_loss = 0\n",
        "    training_accuracy = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        \n",
        "        output = model(text, text_lengths).squeeze()\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        training_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "\n",
        "        acc = num_corrects/len(batch)\n",
        "        training_accuracy += acc.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    \n",
        "    return training_loss / len(iterator), training_accuracy / len(iterator)\n",
        "\n",
        "def testing(model, iterator, optimizer, criterion):\n",
        "    testing_loss = 0\n",
        "    testing_accuracy = 0\n",
        "    model.eval()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        text, text_lengths = batch.text\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = model(text, text_lengths).squeeze()\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            testing_loss += loss.item()\n",
        "            num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "            acc = num_corrects/len(batch)\n",
        "        \n",
        "            testing_accuracy += acc.item()\n",
        "            \n",
        "    return testing_loss / len(iterator), testing_accuracy / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6JnUQS0wTUr"
      },
      "source": [
        "import time\n",
        "\n",
        "n_epochs = 15\n",
        "min_val_loss = float(\"inf\")\n",
        "path='drive/MyDrive/UNT/AG_news/model/saved_weights_cnn.pt'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.3)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = training(model, train_iterator, optimizer, criterion)\n",
        "    val_loss, val_acc = testing(model, valid_iterator, optimizer, criterion)\n",
        "    \n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
        "    print(f'\\tLoss: {val_loss:.4f}(valid)\\t|\\tAcc: {val_acc * 100:.2f}%(valid)')\n",
        "    \n",
        "    if val_loss < min_val_loss:\n",
        "        min_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCYMHwUAwUwY"
      },
      "source": [
        "testing_data=data.TabularDataset(path = 'drive/MyDrive/test.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "testing_iterator = data.BucketIterator(testing_data, batch_size=batch_size,\n",
        "                                                           sort_key=lambda x: len(x.text),\n",
        "                                                           sort_within_batch=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LSR_-f1wdRQ"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    testing_accuracy = 0\n",
        "    model.eval()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        text, text_lengths = batch.text\n",
        "        # text = TEXT.preprocess(text)\n",
        "        label = batch.label\n",
        "        target = torch.autograd.Variable(label).long()\n",
        "        with torch.no_grad():\n",
        "            output = model(text, text_lengths).squeeze()\n",
        "            num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "            acc = num_corrects / len(batch)\n",
        "            testing_accuracy += acc.item()\n",
        "    \n",
        "    return testing_accuracy / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i41Cr_rBbpTA"
      },
      "source": [
        "vocabulary_size = len(TEXT.vocab)\n",
        "n_class = len(LABEL.vocab)\n",
        "embedding_size = 300\n",
        "out_channels = 100\n",
        "kernel_heights = [3, 4, 5]\n",
        "dropout = 0.4\n",
        "\n",
        "cnn = CNNTextClassification(vocabulary_size, embedding_size, max_seq_len,\n",
        "                              out_channels, kernel_heights, dropout, n_class)\n",
        "rnn = RNN(batch_size=BATCH_SIZE, output_size=output_size, hidden_size=32, vocab_size=vocab_size, embedding_length=embedding_dim, weights=embed_lookup)\n",
        "lstm = LSTM(batch_size=BATCH_SIZE, output_size=output_size, hidden_size=32, vocab_size=vocab_size, embedding_length=embedding_dim, weights=embed_lookup)\n",
        "lstm_A = AttentionModel(batch_size=BATCH_SIZE, output_size=2, hidden_size=32, vocab_size=len(train_dataset.get_vocab()), embedding_length=20, weights=embedding_matrix)\n",
        "self_A = SelfAttention(batch_size=BATCH_SIZE, output_size=output_size, hidden_size=32, vocab_size=vocab_size, embedding_length=embedding_dim, weights=embed_lookup)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LjXjhk_bvJq",
        "outputId": "0af1894d-03b5-4378-deba-3aa0b01fe684"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "cnn.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        ...,\n",
              "        [-0.4984,  0.4048, -0.2479,  ..., -0.2578, -0.2369,  0.0047],\n",
              "        [-0.5809,  0.0953, -0.0321,  ...,  0.7431, -0.3895, -0.1347],\n",
              "        [-0.2117,  0.1462,  0.0605,  ..., -0.4895, -0.0707, -0.0814]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQSp-_d8wlOA"
      },
      "source": [
        "cnn.load_state_dict(torch.load(path))\n",
        "rnn.load_state_dict(torch.load(path))\n",
        "lstm.load_state_dict(torch.load(path))\n",
        "lstm_A.load_state_dict(torch.load(path))\n",
        "self_A.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "_CoW5dkYr-ve",
        "outputId": "30960015-e77e-4f9c-f839-0afcdf6999cf"
      },
      "source": [
        "test_acc = predict(cnn, testing_iterator)\n",
        "print(f\"Accuracy {test_acc * 100:.2f}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-983680e896a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy {test_acc * 100:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6nR9FjysBE1"
      },
      "source": [
        "test_acc = predict(rnn, testing_iterator)\n",
        "print(f\"Accuracy {test_acc * 100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N802A7Hvw5ez"
      },
      "source": [
        "test_acc = predict(lstm, testing_iterator)\n",
        "print(f\"Accuracy {test_acc * 100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPcgg5lLw61-"
      },
      "source": [
        "test_acc = predict(lstm_A, testing_iterator)\n",
        "print(f\"Accuracy {test_acc * 100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usOF8_Ljw7dz"
      },
      "source": [
        "test_acc = predict(self_A, testing_iterator)\n",
        "print(f\"Accuracy {test_acc * 100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX31HPTpxGH9"
      },
      "source": [
        "Is it possible to use transformer for the above scenario i.e., Text Sentiment Analysis / Classification. Answer yes/no. If yes implement using the resource below\n",
        "Yes, but I am too tired"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}