{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "ICE_7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTANXkLt80k9"
      },
      "source": [
        "# ICE - 7.\n",
        "# Text Classification with TorchText\n",
        "==================================\n",
        "\n",
        "This tutorial shows how to use the text classification datasets\n",
        "in ``torchtext``, including\n",
        "\n",
        "::\n",
        "\n",
        "   - AG_NEWS,\n",
        "   - SogouNews,\n",
        "   - DBpedia,\n",
        "   - YelpReviewPolarity,\n",
        "   - YelpReviewFull,\n",
        "   - YahooAnswers,\n",
        "   - AmazonReviewPolarity,\n",
        "   - AmazonReviewFull\n",
        "\n",
        "You can download these datasets using Google Searcg they are avilable for free.\n",
        "\n",
        "This example shows how to train a supervised learning algorithm for\n",
        "classification using one of these ``TextClassification`` datasets.\n",
        "\n",
        "Load data with ngrams\n",
        "---------------------\n",
        "\n",
        "Generally speaking, we first need to do preprocessing for any NLP tasks.\n",
        "\n",
        "Here are some items you can remind your self:\n",
        "\n",
        "Build and preprocess dataset:\n",
        "\n",
        "- Segment sentences. Segment words to subwords or characters?\n",
        "- Change words in lower case?\n",
        "- Delete stop words ?\n",
        "- Create special tokens ( i.e. [UNK] [BOS] [EOS] [PAD] ) ?\n",
        "\n",
        "Build vocabulary:\n",
        "\n",
        "- Discard words whose frequencies are under a threshold ?\n",
        "- Build map from word string to index in the embedding table ( str -> int ) \n",
        "- Build label vocabulary\n",
        "- Numericalize words ( Transform list of words to list of numbers )\n",
        "- Choose pad or not ( Using [PAD] )\n",
        "\n",
        "\n",
        "For this specific task, a bag of ngrams feature is applied to capture some partial information\n",
        "about the local word order. In practice, bi-gram or tri-gram are applied\n",
        "to provide more benefits as word groups than only one word. An example:\n",
        "\n",
        "::\n",
        "\n",
        "   For text: **\"load data with ngrams\"**  \n",
        "   1-gram results: \"load\", \"data\", \"with\", \"ngrams\"  \n",
        "   Bi-grams results: \"load data\", \"data with\", \"with ngrams\"  \n",
        "   Tri-grams results: \"load data with\", \"data with ngrams\"\n",
        "\n",
        "``TextClassification`` Dataset supports the ngrams method. By setting\n",
        "ngrams to 2, the example text in the dataset will be a list of single\n",
        "words plus bi-grams string.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Ojzo8hexMg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZZUjkbIvp1E",
        "outputId": "0d0b82a3-2b2d-4cbd-a4e0-9cc830c52393"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install torch>=1.3.1\n",
        "!pip install torchtext==0.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.4\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.9.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2021.5.30)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4) (3.7.4.3)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWRqYOIo80k-",
        "outputId": "b7a166b1-0339-4a9d-ffe3-e47e8fd92856"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "NGRAMS = 1\n",
        "import os\n",
        "if not os.path.isdir('./.data'):\n",
        "\tos.mkdir('./.data')\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ag_news_csv.tar.gz: 100%|██████████| 11.8M/11.8M [00:00<00:00, 72.8MB/s]\n",
            "120000lines [00:05, 22278.44lines/s]\n",
            "120000lines [00:10, 11080.01lines/s]\n",
            "7600lines [00:00, 11147.45lines/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSuAX5YH80lC",
        "outputId": "a981c124-1927-467f-aedc-a7e92b54d8b2"
      },
      "source": [
        "print('One item of training data:', train_dataset[0] ) #(label id, token id tensor)\n",
        "print('Vocabulary size (including ngrams):', len(train_dataset.get_vocab()))\n",
        "print('Class size:', len(train_dataset.get_labels()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One item of training data: (2, tensor([  432,   426,     2,  1606, 14839,   114,    67,     3,   849,    14,\n",
            "           28,    15,    28,    16, 50726,     4,   432,   375,    17,    10,\n",
            "        67508,     7, 52259,     4,    43,  4010,   784,   326,     2]))\n",
            "Vocabulary size (including ngrams): 95812\n",
            "Class size: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1dDw-cf80lF"
      },
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "The model is composed of the\n",
        "`EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__\n",
        "layer and the linear layer (see the figure below). ``nn.EmbeddingBag``\n",
        "computes the mean value of a “bag” of embeddings. The text entries here\n",
        "have different lengths. ``nn.EmbeddingBag`` requires no padding here\n",
        "since the text lengths are saved in offsets.\n",
        "\n",
        "Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n",
        "the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n",
        "performance and memory efficiency to process a sequence of tensors.\n",
        "\n",
        "![](https://github.com/teohangxanh/5290/blob/_static/img/text_sentiment_ngrams_model.png?raw=1)\n",
        "\n",
        "To make \"offsets\" more easy to understand, we put an overview explaination here. \n",
        "\n",
        "The returned values ( text and offsets ) of generate_batch function will be directly used as the input of the model. In the forward pass, they are used as parameters of self.embedding.\n",
        "\n",
        "text is a tensor of shape (N,)  it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), text will be viewed as having B bags. (B is batch_size)\n",
        "\n",
        "For example, assume batch size is 2. In one batch, we have 2 sentences, \"I love python\", \"I love machine learning\"\n",
        "\n",
        "We will first create bag of words for each sentence [\"I\", \"love\", \"python\"], [\"I\", \"love\", \"machine\", \"learning\"]. Then they will be first transformed to word index according to the vocabulary, tensor([2,1,5]),tensor([2,1,4,7]).\n",
        "In this case, then we can concatenate tensor([2,1,5]) and tensor([2,1,4,7]) to get text tensor([2,1,5,2,1,4,7]) of shape (N,) (N=7). We also need to create offsets tensor([0,3]) of shape (B,) (B=2). In tensor([0,3]), 0 means the starting index of first sentence in tensor([2,1,5,2,1,4,7]), and 3 means the starting index of second sentence in tensor([2,1,5,2,1,4,7]).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmosUHpG80lG"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class): #Initilaize modules.\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) # This is equivalent to nn.Embedding followed by torch.mean(dim=0)\n",
        "        self.fc = nn.Linear(embed_dim, num_class) #Use linear layer here. \n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):  # Randomly initilaize parameters\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange) # Uniform distribution\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()  # Make bias zeros \n",
        "\n",
        "    def forward(self, text, offsets): # Foward pass\n",
        "        embedded = self.embedding(text, offsets) # input (N,)  it will be treated as a concatenation of multiple bags (sequences). \n",
        "        # offsets is required to be a 1D tensor containing the starting index positions of each bag in input. \n",
        "        # Therefore, for offsets of shape (B), input will be viewed as having B bags.\n",
        "        # ouput (B, embed_dim)\n",
        "        a =  self.fc(embedded)\n",
        "        return a # ouput (B, num_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kLpXZeL80lJ"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "The AG_NEWS dataset has four labels and therefore the number of classes\n",
        "is four.\n",
        "\n",
        "::\n",
        "\n",
        "   1 : World\n",
        "   2 : Sports\n",
        "   3 : Business\n",
        "   4 : Sci/Tec\n",
        "\n",
        "The vocab size is equal to the length of vocab (including single word\n",
        "and ngrams). The number of classes is equal to the number of labels,\n",
        "which is four in AG_NEWS case.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuQMytxO80lK"
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POrdK4KQ80lN"
      },
      "source": [
        "Functions used to generate batch\n",
        "--------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCf97qME80lN"
      },
      "source": [
        "Since the text entries have different lengths, a custom function\n",
        "generate_batch() is used to generate data batches and offsets. The\n",
        "function is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\n",
        "The input to ``collate_fn`` is a list of tensors with the size of\n",
        "batch_size, and the ``collate_fn`` function packs them into a\n",
        "mini-batch. Pay attention here and make sure that ``collate_fn`` is\n",
        "declared as a top level def. This ensures that the function is available\n",
        "in each worker.\n",
        "\n",
        "The text entries in the original data batch input are packed into a list\n",
        "and concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\n",
        "The offsets is a tensor of delimiters to represent the beginning index\n",
        "of the individual sequence in the text tensor. Label is a tensor saving\n",
        "the labels of individual text entries.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz5A--Hq80lO"
      },
      "source": [
        "def generate_batch(batch): \n",
        "    # Input: a iterator of items with length of batch_size. For example:[(1,(tensor([2,4,3])),(0,tensor([6,5]))]\n",
        "    # Generate a batch used in SGD\n",
        "    label = torch.tensor([entry[0] for entry in batch]) #tensor of shape (batch_size,)\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "    # torch.Tensor.cumsum returns the cumulative sum\n",
        "    # of elements in the dimension dim.\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # For example tensor([0,3])\n",
        "    text = torch.cat(text) # a list of tensor -> tensor of shape (sum([len(i) for i in text]),) For example, tensor([2,4,3,6,5])\n",
        "    return text, offsets, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "913drrJO80lR"
      },
      "source": [
        "Define functions to train the model and evaluate results.\n",
        "---------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jNGiklp80lS"
      },
      "source": [
        "`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n",
        "is recommended for PyTorch users, and it makes data loading in parallel\n",
        "easily (a tutorial is\n",
        "`here <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`__).\n",
        "We use ``DataLoader`` here to load AG_NEWS datasets and send it to the\n",
        "model for training/validation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPjVXDQj80lT"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def train_func(sub_train_):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      collate_fn=generate_batch)  # Iterable batches\n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        optimizer.zero_grad() # Before each optimization, make previous gradients zeros\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        output = model(text, offsets)\n",
        "        loss = criterion(output, cls) # Forward pass to compute loss\n",
        "        train_loss += loss.item()\n",
        "        # Extract the number from a tensor containing only one item, this number will be used in later printing\n",
        "        loss.backward() # Backforward propagation to compute gradients of each variable node\n",
        "        optimizer.step() # Update parameters according to gradients\n",
        "        #choose the class with the highest score as current prediction and compare with gold label (cls )\n",
        "        train_acc += (output.argmax(1) == cls).sum().item() \n",
        "        \n",
        "    # Adjust the learning rate. After each epoch, do learning rate decay ( optional )\n",
        "    scheduler.step()\n",
        "\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_) #return average loss and acc to print\n",
        "\n",
        "def test(data_):\n",
        "    #Similar to train_func but do not need back propagation or parameter update !\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "    for text, offsets, cls in data:\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad(): # prevent computing gradient, could not use backward()\n",
        "            output = model(text, offsets)\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQOFDwuU80lV"
      },
      "source": [
        "Split the dataset and run the model\n",
        "-----------------------------------\n",
        "\n",
        "Since the original AG_NEWS has no valid dataset, we split the training\n",
        "dataset into train/valid sets with a split ratio of 0.95 (train) and\n",
        "0.05 (valid). Here we use\n",
        "`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\n",
        "function in PyTorch core library.\n",
        "\n",
        "`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
        "criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\n",
        "It is useful when training a classification problem with C classes.\n",
        "`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\n",
        "implements stochastic gradient descent method as optimizer. The initial\n",
        "learning rate is set to 4.0.\n",
        "`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\n",
        "is used here to adjust the learning rate through epochs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN0qicW980lW",
        "outputId": "a0d7a134-0559-49d5-ce08-3607d1b656e6"
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "N_EPOCHS = 5\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "#Use CrossEntropyLoss() as the criterion. \n",
        "#The input is the output of the model. First do logsoftmax, then compute cross-entropy loss. \n",
        "criterion = torch.nn.CrossEntropyLoss().to(device) \n",
        "#Use SGD as optimizer.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
        "#Use exponential decay to decrease learning rate\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "train_len = int(len(train_dataset) * 0.95)\n",
        "#Split whole training dataset to create validation (hold-out datset)\n",
        "sub_train_, sub_valid_ = \\\n",
        "    random_split(train_dataset, [train_len, len(train_dataset) - train_len]) \n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(sub_train_)\n",
        "    valid_loss, valid_acc = test(sub_valid_)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "    \n",
        "    #Print information to monitor the training process\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0135(train)\t|\tAcc: 84.5%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 87.1%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0084(train)\t|\tAcc: 91.0%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 89.4%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0071(train)\t|\tAcc: 92.2%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 88.8%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 7 seconds\n",
            "\tLoss: 0.0063(train)\t|\tAcc: 93.2%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 89.8%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 7 seconds\n",
            "\tLoss: 0.0057(train)\t|\tAcc: 93.7%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.0%(valid)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGOh--s80lZ"
      },
      "source": [
        "Running the model on GPU with the following information:\n",
        "\n",
        "Epoch: 1 \\| time in 0 minutes, 11 seconds\n",
        "\n",
        "::\n",
        "\n",
        "       Loss: 0.0263(train)     |       Acc: 84.5%(train)\n",
        "       Loss: 0.0001(valid)     |       Acc: 89.0%(valid)\n",
        "\n",
        "\n",
        "Epoch: 2 \\| time in 0 minutes, 10 seconds\n",
        "\n",
        "::\n",
        "\n",
        "       Loss: 0.0119(train)     |       Acc: 93.6%(train)\n",
        "       Loss: 0.0000(valid)     |       Acc: 89.6%(valid)\n",
        "\n",
        "\n",
        "Epoch: 3 \\| time in 0 minutes, 9 seconds\n",
        "\n",
        "::\n",
        "\n",
        "       Loss: 0.0069(train)     |       Acc: 96.4%(train)\n",
        "       Loss: 0.0000(valid)     |       Acc: 90.5%(valid)\n",
        "\n",
        "\n",
        "Epoch: 4 \\| time in 0 minutes, 11 seconds\n",
        "\n",
        "::\n",
        "\n",
        "       Loss: 0.0038(train)     |       Acc: 98.2%(train)\n",
        "       Loss: 0.0000(valid)     |       Acc: 90.4%(valid)\n",
        "\n",
        "\n",
        "Epoch: 5 \\| time in 0 minutes, 11 seconds\n",
        "\n",
        "::\n",
        "\n",
        "       Loss: 0.0022(train)     |       Acc: 99.0%(train)\n",
        "       Loss: 0.0000(valid)     |       Acc: 91.0%(valid)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0AY2HBG80la"
      },
      "source": [
        "Evaluate the model with test dataset\n",
        "------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpKB9dK_80lb",
        "outputId": "7ac52fad-778b-40bb-e9a7-52e6181c4a38"
      },
      "source": [
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_dataset)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0002(test)\t|\tAcc: 90.4%(test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEShjoB80le"
      },
      "source": [
        "Checking the results of test dataset…\n",
        "\n",
        "::\n",
        "\n",
        "       Loss: 0.0237(test)      |       Acc: 90.5%(test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kCdwqnH80lf"
      },
      "source": [
        "Test on a random news\n",
        "---------------------\n",
        "\n",
        "Use the best model so far and test a golf news. The label information is\n",
        "available\n",
        "`here <https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS>`__.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwENUVtqSTXh"
      },
      "source": [
        "# **Tasks for Today's ICE**\n",
        "## *All Implementation should be perfomed in Pytorch/torchtext*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mOT7EpHRaaw"
      },
      "source": [
        "# The code for the models is provided as an attachment as Zip file. Please use that as reference to perform classification in terms of accuracy and then test any random piece of code\n",
        "# as test data to idetify what the article is talking about i.e., \"This is Political News\". You can use any dataset that are introduced in the start\n",
        "# All datasets are available for free using google search.\n",
        "# Final step is to compare the accuracies and provide discussion on why one model has a better performance while do not. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaF5pg6S3dhB"
      },
      "source": [
        "Answer: Attention models usually give the highest accuracy because its function mainly gives importance to some input states in which it has more contextual relation. So, generally the weights for the inputs of attention function are learned to understand which input it should attend to.\n",
        "A variant of attention models, self attention is considerable because: <br>\n",
        "* Minimize total computational complexity per layer\n",
        "* Maximize amount of parallelizable computations, measured by minimum number of sequential operations required\n",
        "* Minimize maximum path length between any two input and output positions in network composed of the different layer types . The shorter the path between any combination of positions in the input and * output sequences, the easier to learn long-range dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A8PkKmnWDJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16da5456-9774-40e5-865c-2d65cb64c095"
      },
      "source": [
        "import pandas as pd\n",
        "!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "glove = pd.read_csv('glove.6B.50d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
        "glove_embedding = {key: val.values for key, val in glove.T.items()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-19 19:59:18--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2021-10-19 19:59:18--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2021-10-19 19:59:19--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 40s  \n",
            "\n",
            "2021-10-19 20:01:59 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kpHP1Xz4CMj"
      },
      "source": [
        "ex_text_str = \"The surface area of a lone black hole won’t change — after all,\\\n",
        " nothing can escape from within. However, if you throw something into a black \\\n",
        " hole, it will gain more mass, increasing its surface area. But the incoming \\\n",
        " object could also make the black hole spin, which decreases the surface area.\\\n",
        "  The area law says that the increase in surface area due to additional mass \\\n",
        "  will always outweigh the decrease in surface area due to added spin.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykaod9eBHRZ"
      },
      "source": [
        "import torchtext\n",
        "import numpy as np\n",
        "\n",
        "matrix_len = 2000\n",
        "weights_matrix = np.zeros((matrix_len, 50))\n",
        "words_found = 0\n",
        "count = 0\n",
        "for i, word in enumerate(train_dataset.get_vocab()):\n",
        "  if count >= matrix_len:\n",
        "    break\n",
        "  try: \n",
        "      weights_matrix[i] = glove_embedding[word]\n",
        "      print(glove[word])\n",
        "      words_found += 1\n",
        "  except KeyError:\n",
        "      weights_matrix[i] = np.random.normal(scale=0.6, size=(50, ))\n",
        "  count += 1\n",
        "\n",
        "weights_matrix = torch.from_numpy(weights_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5aGHbtwjVNd"
      },
      "source": [
        "def define(model):\n",
        "  optimizer = Adam(model.parameters(), lr=0.07)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  if torch.cude.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "  print('Print model')\n",
        "  print(model)\n",
        "\n",
        "def train(epoch, model, X_train, y_train, X_val, y_val):\n",
        "  model.train()\n",
        "  tr_loss = 0\n",
        "  if torch.cuda.is_available():\n",
        "    X_train = X_train.cuda()\n",
        "    y_train = y_train.cuda()\n",
        "    X_val = X_val.cuda()\n",
        "    y_val = y_val.cuda()\n",
        "\n",
        "  # Clearing the gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Prediction for train and val set\n",
        "  output_train = model(X_train)\n",
        "  output_val = model(X_val)\n",
        "\n",
        "  # Compute train and val loss\n",
        "  loss_train = criterion(output_train, y_train)\n",
        "  loss_val = criterion(output_val, y_val)\n",
        "  train_losses.append(loss_train)\n",
        "  val_losses.append(loss_val)\n",
        "\n",
        "  # Compute the updated weights of all model parameters\n",
        "  loss_train.backward()\n",
        "  optimizer.step()\n",
        "  tr_loss = loss_train.item()\n",
        "  if epoch % (epoch // 10) == 0:\n",
        "    print(f'Epoch: {epoch + 1},   loss : {loss_val}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwBc7vcFP64v"
      },
      "source": [
        "# Use CNN for the above scenario\n",
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, in_channels, out_channels, kernel_heights, stride, padding, keep_probab, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(CNN, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of each batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\tin_channels : Number of input channels. Here it is 1 as the input data has dimension = (batch_size, num_seq, embedding_length)\n",
        "\t\tout_channels : Number of output channels after convolution operation performed on the input matrix\n",
        "\t\tkernel_heights : A list consisting of 3 different kernel_heights. Convolution will be performed 3 times and finally results from each kernel_height will be concatenated.\n",
        "\t\tkeep_probab : Probability of retaining an activation node during dropout operation\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embedding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.in_channels = in_channels\n",
        "\t\tself.out_channels = out_channels\n",
        "\t\tself.kernel_heights = kernel_heights\n",
        "\t\tself.stride = stride\n",
        "\t\tself.padding = padding\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.conv1 = nn.Conv2d(in_channels, out_channels, (kernel_heights[0], embedding_length), stride, padding)\n",
        "\t\tself.conv2 = nn.Conv2d(in_channels, out_channels, (kernel_heights[1], embedding_length), stride, padding)\n",
        "\t\tself.conv3 = nn.Conv2d(in_channels, out_channels, (kernel_heights[2], embedding_length), stride, padding)\n",
        "\t\tself.dropout = nn.Dropout(keep_probab)\n",
        "\t\tself.label = nn.Linear(len(kernel_heights)*out_channels, output_size)\n",
        "\t\n",
        "\tdef conv_block(self, input, conv_layer):\n",
        "\t\tconv_out = conv_layer(input)# conv_out.size() = (batch_size, out_channels, dim, 1)\n",
        "\t\tactivation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n",
        "\t\tmax_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n",
        "\t\t\n",
        "\t\treturn max_out\n",
        "\t\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tThe idea of the Convolutional Neural Netwok for Text Classification is very simple. We perform convolution operation on the embedding matrix \n",
        "\t\twhose shape for each batch is (num_seq, embedding_length) with kernel of varying height but constant width which is same as the embedding_length.\n",
        "\t\tWe will be using ReLU activation after the convolution operation and then for each kernel height, we will use max_pool operation on each tensor \n",
        "\t\tand will filter all the maximum activation for every channel and then we will concatenate the resulting tensors. This output is then fully connected\n",
        "\t\tto the output layers consisting two units which basically gives us the logits for both positive and negative classes.\n",
        "\t\t\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentences: input_sentences of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
        "\t\tlogits.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\t# input.size() = (batch_size, num_seq, embedding_length)\n",
        "\t\tinput = input.unsqueeze(1)\n",
        "\t\t# input.size() = (batch_size, 1, num_seq, embedding_length)\n",
        "\t\tmax_out1 = self.conv_block(input, self.conv1)\n",
        "\t\tmax_out2 = self.conv_block(input, self.conv2)\n",
        "\t\tmax_out3 = self.conv_block(input, self.conv3)\n",
        "\t\t\n",
        "\t\tall_out = torch.cat((max_out1, max_out2, max_out3), 1)\n",
        "\t\t# all_out.size() = (batch_size, num_kernels*out_channels)\n",
        "\t\tfc_in = self.dropout(all_out)\n",
        "\t\t# fc_in.size()) = (batch_size, num_kernels*out_channels)\n",
        "\t\tlogits = self.label(fc_in)\n",
        "\t\t\n",
        "\t\treturn logits\n",
        "\n",
        "vocab_size = 2000\n",
        "output_size = 2\n",
        "in_channels = 1\n",
        "out_channels = 4\n",
        "num_filters = [2, 2, 2]\n",
        "kernel_heights = [2, 2, 2]\n",
        "\n",
        "cnn = CNN(batch_size=BATCH_SIZE, output_size=output_size, in_channels=in_channels, out_channels=out_channels, \\\n",
        "          kernel_heights=kernel_heights, stride=1, padding=0, keep_probab=0.5, vocab_size=vocab_size, \\\n",
        "\t\t\t\t\t     embedding_length=len(glove), weights=weights_matrix)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-4)\n",
        "for t in range(500):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = cnn(text)\n",
        "\n",
        "    # Compute and print loss\n",
        "    rnn_loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, cnn, vocab, 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5JOdmG_P_Ho"
      },
      "source": [
        "# Use RNN for the above scenario\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(RNN, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.rnn = nn.RNN(embedding_length, hidden_size, num_layers=2, bidirectional=True)\n",
        "\t\tself.label = nn.Linear(4*hidden_size, output_size)\n",
        "\t\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\t\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the final_hidden_state of RNN.\n",
        "\t\tlogits.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda()) # 4 = num_layers*num_directions\n",
        "\t\telse:\n",
        "\t\t\th_0 =  Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, h_n = self.rnn(input, h_0)\n",
        "\t\t# h_n.size() = (4, batch_size, hidden_size)\n",
        "\t\th_n = h_n.permute(1, 0, 2) # h_n.size() = (batch_size, 4, hidden_size)\n",
        "\t\th_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
        "\t\t# h_n.size() = (batch_size, 4*hidden_size)\n",
        "\t\tlogits = self.label(h_n) # logits.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\treturn logits\n",
        "\n",
        "    \n",
        "rnn = RNN(batch_size=BATCH_SIZE, output_size=output_size, hidden_size=32, vocab_size=vocab_size, embedding_length=embedding_dim, weights=embed_lookup)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=1e-4)\n",
        "for t in range(500):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = rnn(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    rnn_loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, rnn, vocab, 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B3F5GQiQA8N"
      },
      "source": [
        "# Use LSTM for the above scenario\n",
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(LSTMClassifier, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t\n",
        "\tdef forward(self, input_sentence, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
        "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
        "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\treturn final_output\n",
        "\n",
        "\n",
        "lstm = LSTM(batch_size=BATCH_SIZE, output_size=output_size, hidden_size=32, vocab_size=vocab_size, embedding_length=embedding_dim, weights=embed_lookup)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(lstm.parameters(), lr=1e-4)\n",
        "for t in range(500):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = lstm(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    rnn_loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, lstm, vocab, 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk8L1TO6QGDm"
      },
      "source": [
        "# Use LSTM Attention for the above scenario\n",
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "class AttentionModel(torch.nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(AttentionModel, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t#self.attn_fc_layer = nn.Linear()\n",
        "\t\t\n",
        "\tdef attention_net(self, lstm_output, final_state):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tNow we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
        "\t\tbetween each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
        "\t\t\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tlstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
        "\t\tfinal_state : Final time-step hidden state (h_n) of the LSTM\n",
        "\t\t\n",
        "\t\t---------\n",
        "\t\t\n",
        "\t\tReturns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
        "\t\t\t\t  new hidden state.\n",
        "\t\t\t\t  \n",
        "\t\tTensor Size :\n",
        "\t\t\t\t\thidden.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\tattn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tsoft_attn_weights.size() = (batch_size, num_seq)\n",
        "\t\t\t\t\tnew_hidden_state.size() = (batch_size, hidden_size)\n",
        "\t\t\t\t\t  \n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\thidden = final_state.squeeze(0)\n",
        "\t\tattn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "\t\tsoft_attn_weights = F.softmax(attn_weights, 1)\n",
        "\t\tnew_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "\t\t\n",
        "\t\treturn new_hidden_state\n",
        "\t\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\t\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \n",
        "\t\toutput = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
        "\t\t\n",
        "\t\tattn_output = self.attention_net(output, final_hidden_state)\n",
        "\t\tlogits = self.label(attn_output)\n",
        "\t\t\n",
        "\t\treturn logits\n",
        "\n",
        "\n",
        "lstm_A = AttentionModel(batch_size=BATCH_SIZE, output_size=2, hidden_size=32, vocab_size=len(train_dataset.get_vocab()), embedding_length=20, weights=embedding_matrix)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(lstm_A.parameters(), lr=1e-4)\n",
        "for t in range(500):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = lstm_A(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    rnn_loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, lstm_A, vocab, 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_76d82rwQIy9"
      },
      "source": [
        "# Use Self Attention for the above scenario\n",
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(SelfAttention, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\tself.weights = weights\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.dropout = 0.8\n",
        "\t\tself.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
        "\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
        "\t\tself.W_s1 = nn.Linear(2*hidden_size, 350)\n",
        "\t\tself.W_s2 = nn.Linear(350, 30)\n",
        "\t\tself.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
        "\t\tself.label = nn.Linear(2000, output_size)\n",
        "\n",
        "\tdef attention_net(self, lstm_output):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
        "\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n",
        "\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n",
        "\t\tconnected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n",
        "\t\tpos & neg.\n",
        "\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\n",
        "\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
        "\t\t---------\n",
        "\n",
        "\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
        "\t\t\t\t  attention to different parts of the input sentence.\n",
        "\n",
        "\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tattn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
        "\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "\n",
        "\t\treturn attn_weight_matrix\n",
        "\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tinput = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "\t\toutput, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
        "\t\toutput = output.permute(1, 0, 2)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t# h_n.size() = (1, batch_size, hidden_size)\n",
        "\t\t# c_n.size() = (1, batch_size, hidden_size)\n",
        "\t\tattn_weight_matrix = self.attention_net(output)\n",
        "\t\t# attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\thidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
        "\t\t# hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
        "\t\t# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
        "\t\tfc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
        "\t\tlogits = self.label(fc_out)\n",
        "\t\t# logits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\treturn logits\n",
        "\n",
        "self_A = SelfAttention(batch_size=BATCH_SIZE, output_size=output_size, hidden_size=32, vocab_size=vocab_size, embedding_length=embedding_dim, weights=embed_lookup)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(self_A.parameters(), lr=1e-4)\n",
        "for t in range(500):\n",
        "# Forward pass: Compute predicted y by passing x to the model\n",
        "y_pred = self_A(x)\n",
        "\n",
        "# Compute and print loss\n",
        "self_A_loss = criterion(y_pred, y)\n",
        "if t % 100 == 99:\n",
        "    print(t, loss.item())\n",
        "\n",
        "# Zero gradients, perform a backward pass, and update the weights.\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, self_A, vocab, 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGJXel5xQO_v"
      },
      "source": [
        "###### Is it possible to use transformer for the above scenario i.e., Text Sentiment Analysis / Classification. Answer yes/no. If yes implement using the resource below\n",
        "Yes, but I am too tired\n",
        "###### If No why do you think it is not possible for transformer to perform text classification in PyTorch.\n",
        "###### You can take help from https://github.com/Renovamen/Text-Classification/tree/master/models/Transformer to implement transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy5HQrna3dhD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}