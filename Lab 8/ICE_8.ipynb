{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICE_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9v-D6hh8u0S"
      },
      "source": [
        "# ICE-8\n",
        "# This ICE is pretty straight forward. Please follow the link below and solve all the coding problems that are explained\n",
        "# https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
        "# Once done with the above part, use any dataset of your choice and build your own language model with the techniques covered in the online tutorial.\n",
        "\n",
        "# (50%)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nm76ryUVg0e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import re\n",
        "\n",
        "data_text = my_text = \"\"\"\n",
        "As I have noted repeatedly, liberalism and leftism have virtually nothing in common. In fact, leftism is the enemy of liberalism — as a handful of liberals such as former New York Times writer Bari Weiss, former Young Turk Dave Rubin, and others have come to recognize. \n",
        "\n",
        "The left has never believed in free speech and has suppressed dissent wherever it has assumed power. Free speech is a pillar of liberalism, and it has always embraced dissent.\n",
        "\n",
        "The left rejects the anti-racist ideal of color-blindness. Colorblind is the liberal racial ideal.\n",
        "\n",
        "The left supports racial segregation — such as all-black dorms and separate black graduations. Liberals have always advocated racial integration.\n",
        "\n",
        "The left has always loathed capitalism. Liberals were always major advocates of capitalism — recognizing that only capitalism has lifted billions of people out of poverty. \n",
        "\n",
        "The left has always been anti-Israel. Liberals have always been fervent supporters of Israel. \n",
        "\n",
        "The left has always held America in contempt. Liberals loved this country. A liberal wrote, “God bless America.” No leftist would write such a song.\n",
        "\n",
        "Leftists want to defund the police. No liberal does.\n",
        "\n",
        "The list of liberal-left differences is as long as the list of left-wing positions.\n",
        "\n",
        "Yet, it is liberals who keep the left in power. Were it not for the liberal vote, the left would have no power.\n",
        "\n",
        "The question is all the more apt given that it is conservatives who protect virtually every liberal value. It is conservatives who seek to preserve free speech, racial integration, love of America, a strong Israel, and capitalism.\n",
        "\n",
        "So why do liberals vote for the left, for the very people who hold liberals and their values in contempt?\n",
        "\n",
        "There are two primary reasons.\n",
        "\n",
        "One is brainwash. Liberals are brainwashed from childhood into believing that the right is their enemy and that pas d’ennemis a gauche (there are “no enemies on the left”). That is why there is no left-wing position, no matter how destructive or vile, that could move a liberal to vote Republican or identify with conservatives.\n",
        "\n",
        "The second reason is fear. Liberals fear they will lose friends and even family if they do not vote Democrat or if they publicly criticize the left. And this is not an irrational fear. \n",
        "\n",
        "America and the West are being destroyed by the left. But this destruction of the universities, the high schools, art and music, journalism, and of freedom itself could not take place were it not for liberals.\n",
        "\n",
        "The fate of America and the West lies largely in the hands of liberals. There are simply not enough leftists to destroy our most revered institutions. They need liberals to serve as fellow travelers to accomplish their ends. \n",
        "\n",
        "Should the American experiment fail — and it may — that profile in lack of courage, the liberal, will have made it possible. \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def text_cleaner(text):\n",
        "    # lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\", \"\", newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    long_words = []\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i) >= 3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "\n",
        "# preprocess the text\n",
        "data_new = text_cleaner(data_text)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVPjY9ZNVg0j",
        "outputId": "d4776de7-c969-4cf3-8e81-cfe3c6bf1820"
      },
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        # select sequence of tokens\n",
        "        seq = text[i - length : i + 1]\n",
        "        # store\n",
        "        sequences.append(seq)\n",
        "    print(\"Total Sequences: %d\" % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "\n",
        "# create sequences\n",
        "sequences = create_seq(data_new)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 2440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7DgLODVg0l"
      },
      "source": [
        "# create a character mapping index\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqUwG4TrVg0m",
        "outputId": "11d2f4d5-141a-4325-f329-95e00bd870f6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"Train shape:\", X_tr.shape, \"Val shape:\", X_val.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (2196, 30) Val shape: (244, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm4pymHGVg0m",
        "outputId": "cc471e9a-a4da-4a7c-c475-9847a77173e8"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 50)            1350      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 150)               90900     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 27)                4077      \n",
            "=================================================================\n",
            "Total params: 96,327\n",
            "Trainable params: 96,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "69/69 - 19s - loss: 2.9825 - acc: 0.1526 - val_loss: 2.7990 - val_acc: 0.1557\n",
            "Epoch 2/100\n",
            "69/69 - 14s - loss: 2.7334 - acc: 0.2017 - val_loss: 2.5342 - val_acc: 0.2828\n",
            "Epoch 3/100\n",
            "69/69 - 14s - loss: 2.4324 - acc: 0.2864 - val_loss: 2.3276 - val_acc: 0.3320\n",
            "Epoch 4/100\n",
            "69/69 - 14s - loss: 2.2674 - acc: 0.3411 - val_loss: 2.2123 - val_acc: 0.3648\n",
            "Epoch 5/100\n",
            "69/69 - 14s - loss: 2.1647 - acc: 0.3784 - val_loss: 2.1548 - val_acc: 0.3852\n",
            "Epoch 6/100\n",
            "69/69 - 14s - loss: 2.0631 - acc: 0.3925 - val_loss: 2.0873 - val_acc: 0.4016\n",
            "Epoch 7/100\n",
            "69/69 - 14s - loss: 1.9829 - acc: 0.4135 - val_loss: 2.0429 - val_acc: 0.4098\n",
            "Epoch 8/100\n",
            "69/69 - 14s - loss: 1.9167 - acc: 0.4331 - val_loss: 2.0227 - val_acc: 0.4057\n",
            "Epoch 9/100\n",
            "69/69 - 14s - loss: 1.8319 - acc: 0.4458 - val_loss: 1.9999 - val_acc: 0.4016\n",
            "Epoch 10/100\n",
            "69/69 - 14s - loss: 1.7739 - acc: 0.4745 - val_loss: 1.9995 - val_acc: 0.4262\n",
            "Epoch 11/100\n",
            "69/69 - 14s - loss: 1.6967 - acc: 0.4891 - val_loss: 1.9363 - val_acc: 0.4467\n",
            "Epoch 12/100\n",
            "69/69 - 14s - loss: 1.6348 - acc: 0.5059 - val_loss: 1.9752 - val_acc: 0.4549\n",
            "Epoch 13/100\n",
            "69/69 - 14s - loss: 1.5754 - acc: 0.5255 - val_loss: 1.9400 - val_acc: 0.4672\n",
            "Epoch 14/100\n",
            "69/69 - 14s - loss: 1.5053 - acc: 0.5373 - val_loss: 1.9511 - val_acc: 0.4549\n",
            "Epoch 15/100\n",
            "69/69 - 14s - loss: 1.4347 - acc: 0.5729 - val_loss: 1.9562 - val_acc: 0.4590\n",
            "Epoch 16/100\n",
            "69/69 - 14s - loss: 1.3537 - acc: 0.5929 - val_loss: 1.9459 - val_acc: 0.4918\n",
            "Epoch 17/100\n",
            "69/69 - 14s - loss: 1.2987 - acc: 0.6084 - val_loss: 1.9145 - val_acc: 0.5041\n",
            "Epoch 18/100\n",
            "69/69 - 14s - loss: 1.2271 - acc: 0.6280 - val_loss: 1.9725 - val_acc: 0.5000\n",
            "Epoch 19/100\n",
            "69/69 - 14s - loss: 1.1544 - acc: 0.6494 - val_loss: 1.9743 - val_acc: 0.5000\n",
            "Epoch 20/100\n",
            "69/69 - 14s - loss: 1.0969 - acc: 0.6658 - val_loss: 1.9895 - val_acc: 0.4877\n",
            "Epoch 21/100\n",
            "69/69 - 14s - loss: 1.0287 - acc: 0.6958 - val_loss: 2.0019 - val_acc: 0.5082\n",
            "Epoch 22/100\n",
            "69/69 - 14s - loss: 0.9757 - acc: 0.7017 - val_loss: 1.9897 - val_acc: 0.4959\n",
            "Epoch 23/100\n",
            "69/69 - 14s - loss: 0.9190 - acc: 0.7131 - val_loss: 1.9971 - val_acc: 0.5041\n",
            "Epoch 24/100\n",
            "69/69 - 14s - loss: 0.8624 - acc: 0.7423 - val_loss: 2.0254 - val_acc: 0.5082\n",
            "Epoch 25/100\n",
            "69/69 - 14s - loss: 0.8198 - acc: 0.7500 - val_loss: 2.0445 - val_acc: 0.5205\n",
            "Epoch 26/100\n",
            "69/69 - 14s - loss: 0.7572 - acc: 0.7705 - val_loss: 2.0415 - val_acc: 0.5082\n",
            "Epoch 27/100\n",
            "69/69 - 14s - loss: 0.7205 - acc: 0.7864 - val_loss: 2.1040 - val_acc: 0.5205\n",
            "Epoch 28/100\n",
            "69/69 - 14s - loss: 0.6847 - acc: 0.7983 - val_loss: 2.1064 - val_acc: 0.5287\n",
            "Epoch 29/100\n",
            "69/69 - 14s - loss: 0.6442 - acc: 0.8110 - val_loss: 2.1177 - val_acc: 0.5246\n",
            "Epoch 30/100\n",
            "69/69 - 14s - loss: 0.6056 - acc: 0.8251 - val_loss: 2.1456 - val_acc: 0.5164\n",
            "Epoch 31/100\n",
            "69/69 - 14s - loss: 0.5659 - acc: 0.8393 - val_loss: 2.1612 - val_acc: 0.5246\n",
            "Epoch 32/100\n",
            "69/69 - 14s - loss: 0.5375 - acc: 0.8493 - val_loss: 2.1971 - val_acc: 0.5041\n",
            "Epoch 33/100\n",
            "69/69 - 14s - loss: 0.5231 - acc: 0.8488 - val_loss: 2.1871 - val_acc: 0.4959\n",
            "Epoch 34/100\n",
            "69/69 - 14s - loss: 0.4791 - acc: 0.8684 - val_loss: 2.1768 - val_acc: 0.5123\n",
            "Epoch 35/100\n",
            "69/69 - 14s - loss: 0.4501 - acc: 0.8748 - val_loss: 2.2493 - val_acc: 0.5123\n",
            "Epoch 36/100\n",
            "69/69 - 14s - loss: 0.4313 - acc: 0.8912 - val_loss: 2.2947 - val_acc: 0.5082\n",
            "Epoch 37/100\n",
            "69/69 - 14s - loss: 0.4033 - acc: 0.8889 - val_loss: 2.2953 - val_acc: 0.5123\n",
            "Epoch 38/100\n",
            "69/69 - 14s - loss: 0.3938 - acc: 0.8980 - val_loss: 2.3234 - val_acc: 0.5041\n",
            "Epoch 39/100\n",
            "69/69 - 14s - loss: 0.3786 - acc: 0.8962 - val_loss: 2.3651 - val_acc: 0.5000\n",
            "Epoch 40/100\n",
            "69/69 - 14s - loss: 0.3455 - acc: 0.9076 - val_loss: 2.4226 - val_acc: 0.4959\n",
            "Epoch 41/100\n",
            "69/69 - 14s - loss: 0.3322 - acc: 0.9171 - val_loss: 2.4158 - val_acc: 0.5000\n",
            "Epoch 42/100\n",
            "69/69 - 15s - loss: 0.3146 - acc: 0.9176 - val_loss: 2.4975 - val_acc: 0.5082\n",
            "Epoch 43/100\n",
            "69/69 - 15s - loss: 0.3133 - acc: 0.9185 - val_loss: 2.5056 - val_acc: 0.5123\n",
            "Epoch 44/100\n",
            "69/69 - 15s - loss: 0.2939 - acc: 0.9194 - val_loss: 2.4882 - val_acc: 0.5205\n",
            "Epoch 45/100\n",
            "69/69 - 15s - loss: 0.2784 - acc: 0.9290 - val_loss: 2.5223 - val_acc: 0.5205\n",
            "Epoch 46/100\n",
            "69/69 - 15s - loss: 0.2729 - acc: 0.9258 - val_loss: 2.5326 - val_acc: 0.5041\n",
            "Epoch 47/100\n",
            "69/69 - 15s - loss: 0.2675 - acc: 0.9281 - val_loss: 2.5880 - val_acc: 0.5082\n",
            "Epoch 48/100\n",
            "69/69 - 15s - loss: 0.2541 - acc: 0.9340 - val_loss: 2.5770 - val_acc: 0.5123\n",
            "Epoch 49/100\n",
            "69/69 - 15s - loss: 0.2509 - acc: 0.9372 - val_loss: 2.6151 - val_acc: 0.5164\n",
            "Epoch 50/100\n",
            "69/69 - 15s - loss: 0.2337 - acc: 0.9422 - val_loss: 2.6121 - val_acc: 0.5082\n",
            "Epoch 51/100\n",
            "69/69 - 14s - loss: 0.2269 - acc: 0.9422 - val_loss: 2.7032 - val_acc: 0.5000\n",
            "Epoch 52/100\n",
            "69/69 - 14s - loss: 0.2249 - acc: 0.9399 - val_loss: 2.7505 - val_acc: 0.5205\n",
            "Epoch 53/100\n",
            "69/69 - 15s - loss: 0.2119 - acc: 0.9435 - val_loss: 2.7001 - val_acc: 0.5082\n",
            "Epoch 54/100\n",
            "69/69 - 15s - loss: 0.2124 - acc: 0.9440 - val_loss: 2.7340 - val_acc: 0.5123\n",
            "Epoch 55/100\n",
            "69/69 - 14s - loss: 0.1922 - acc: 0.9522 - val_loss: 2.7566 - val_acc: 0.5041\n",
            "Epoch 56/100\n",
            "69/69 - 14s - loss: 0.1993 - acc: 0.9476 - val_loss: 2.7908 - val_acc: 0.5123\n",
            "Epoch 57/100\n",
            "69/69 - 14s - loss: 0.1902 - acc: 0.9485 - val_loss: 2.8514 - val_acc: 0.5041\n",
            "Epoch 58/100\n",
            "69/69 - 15s - loss: 0.1850 - acc: 0.9540 - val_loss: 2.8740 - val_acc: 0.5041\n",
            "Epoch 59/100\n",
            "69/69 - 14s - loss: 0.1786 - acc: 0.9508 - val_loss: 2.8784 - val_acc: 0.5041\n",
            "Epoch 60/100\n",
            "69/69 - 14s - loss: 0.1676 - acc: 0.9536 - val_loss: 2.8809 - val_acc: 0.5082\n",
            "Epoch 61/100\n",
            "69/69 - 14s - loss: 0.1710 - acc: 0.9536 - val_loss: 2.8749 - val_acc: 0.5082\n",
            "Epoch 62/100\n",
            "69/69 - 14s - loss: 0.1763 - acc: 0.9513 - val_loss: 2.8937 - val_acc: 0.5000\n",
            "Epoch 63/100\n",
            "69/69 - 15s - loss: 0.1581 - acc: 0.9604 - val_loss: 2.9659 - val_acc: 0.5041\n",
            "Epoch 64/100\n",
            "69/69 - 14s - loss: 0.1650 - acc: 0.9536 - val_loss: 3.0246 - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            "69/69 - 15s - loss: 0.1530 - acc: 0.9613 - val_loss: 2.9794 - val_acc: 0.5082\n",
            "Epoch 66/100\n",
            "69/69 - 14s - loss: 0.1546 - acc: 0.9581 - val_loss: 3.0372 - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            "69/69 - 14s - loss: 0.1445 - acc: 0.9663 - val_loss: 3.0169 - val_acc: 0.5082\n",
            "Epoch 68/100\n",
            "69/69 - 15s - loss: 0.1557 - acc: 0.9554 - val_loss: 3.0536 - val_acc: 0.5082\n",
            "Epoch 69/100\n",
            "69/69 - 15s - loss: 0.1412 - acc: 0.9622 - val_loss: 3.0652 - val_acc: 0.5000\n",
            "Epoch 70/100\n",
            "69/69 - 15s - loss: 0.1353 - acc: 0.9649 - val_loss: 3.1124 - val_acc: 0.4959\n",
            "Epoch 71/100\n",
            "69/69 - 14s - loss: 0.1409 - acc: 0.9599 - val_loss: 3.1282 - val_acc: 0.5000\n",
            "Epoch 72/100\n",
            "69/69 - 14s - loss: 0.1281 - acc: 0.9658 - val_loss: 3.1514 - val_acc: 0.5082\n",
            "Epoch 73/100\n",
            "69/69 - 15s - loss: 0.1372 - acc: 0.9631 - val_loss: 3.1497 - val_acc: 0.5041\n",
            "Epoch 74/100\n",
            "69/69 - 14s - loss: 0.1398 - acc: 0.9577 - val_loss: 3.2023 - val_acc: 0.5000\n",
            "Epoch 75/100\n",
            "69/69 - 14s - loss: 0.1228 - acc: 0.9686 - val_loss: 3.1745 - val_acc: 0.5041\n",
            "Epoch 76/100\n",
            "69/69 - 14s - loss: 0.1290 - acc: 0.9631 - val_loss: 3.1359 - val_acc: 0.5041\n",
            "Epoch 77/100\n",
            "69/69 - 14s - loss: 0.1277 - acc: 0.9654 - val_loss: 3.2173 - val_acc: 0.4959\n",
            "Epoch 78/100\n",
            "69/69 - 14s - loss: 0.1261 - acc: 0.9622 - val_loss: 3.2584 - val_acc: 0.5082\n",
            "Epoch 79/100\n",
            "69/69 - 14s - loss: 0.1339 - acc: 0.9613 - val_loss: 3.1907 - val_acc: 0.5123\n",
            "Epoch 80/100\n",
            "69/69 - 15s - loss: 0.1274 - acc: 0.9631 - val_loss: 3.2346 - val_acc: 0.5082\n",
            "Epoch 81/100\n",
            "69/69 - 14s - loss: 0.1141 - acc: 0.9690 - val_loss: 3.3037 - val_acc: 0.5123\n",
            "Epoch 82/100\n",
            "69/69 - 15s - loss: 0.1117 - acc: 0.9695 - val_loss: 3.2644 - val_acc: 0.5041\n",
            "Epoch 83/100\n",
            "69/69 - 14s - loss: 0.1073 - acc: 0.9704 - val_loss: 3.3114 - val_acc: 0.4959\n",
            "Epoch 84/100\n",
            "69/69 - 15s - loss: 0.1015 - acc: 0.9731 - val_loss: 3.3331 - val_acc: 0.5000\n",
            "Epoch 85/100\n",
            "69/69 - 14s - loss: 0.1042 - acc: 0.9704 - val_loss: 3.3796 - val_acc: 0.4918\n",
            "Epoch 86/100\n",
            "69/69 - 14s - loss: 0.0970 - acc: 0.9754 - val_loss: 3.3557 - val_acc: 0.5000\n",
            "Epoch 87/100\n",
            "69/69 - 14s - loss: 0.1082 - acc: 0.9686 - val_loss: 3.4298 - val_acc: 0.4959\n",
            "Epoch 88/100\n",
            "69/69 - 15s - loss: 0.1054 - acc: 0.9690 - val_loss: 3.4389 - val_acc: 0.4959\n",
            "Epoch 89/100\n",
            "69/69 - 14s - loss: 0.1078 - acc: 0.9686 - val_loss: 3.4278 - val_acc: 0.4836\n",
            "Epoch 90/100\n",
            "69/69 - 14s - loss: 0.1033 - acc: 0.9722 - val_loss: 3.4571 - val_acc: 0.4959\n",
            "Epoch 91/100\n",
            "69/69 - 14s - loss: 0.0967 - acc: 0.9740 - val_loss: 3.5014 - val_acc: 0.4836\n",
            "Epoch 92/100\n",
            "69/69 - 14s - loss: 0.0918 - acc: 0.9718 - val_loss: 3.5069 - val_acc: 0.4836\n",
            "Epoch 93/100\n",
            "69/69 - 14s - loss: 0.1025 - acc: 0.9690 - val_loss: 3.5645 - val_acc: 0.4918\n",
            "Epoch 94/100\n",
            "69/69 - 14s - loss: 0.0987 - acc: 0.9750 - val_loss: 3.5239 - val_acc: 0.4713\n",
            "Epoch 95/100\n",
            "69/69 - 15s - loss: 0.0929 - acc: 0.9727 - val_loss: 3.5306 - val_acc: 0.4877\n",
            "Epoch 96/100\n",
            "69/69 - 14s - loss: 0.0899 - acc: 0.9763 - val_loss: 3.5544 - val_acc: 0.4836\n",
            "Epoch 97/100\n",
            "69/69 - 14s - loss: 0.0950 - acc: 0.9731 - val_loss: 3.5526 - val_acc: 0.4713\n",
            "Epoch 98/100\n",
            "69/69 - 14s - loss: 0.0985 - acc: 0.9713 - val_loss: 3.5804 - val_acc: 0.4836\n",
            "Epoch 99/100\n",
            "69/69 - 14s - loss: 0.1022 - acc: 0.9695 - val_loss: 3.6342 - val_acc: 0.4631\n",
            "Epoch 100/100\n",
            "69/69 - 14s - loss: 0.0961 - acc: 0.9718 - val_loss: 3.6375 - val_acc: 0.4754\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faec0043a10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TJrWYndWZ0y"
      },
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict character\n",
        "\t\tyhat = np.argmax(model.predict(encoded), axis=-1)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw1h64x4Way2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2781d4ff-ae59-4f03-fed4-f0565eea9bba"
      },
      "source": [
        "to_be_written = 'i am afraid that'\n",
        "print(generate_seq(model, mapping, 50, to_be_written.lower(), 100))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 30) for input KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 50).\n",
            "i am afraid that profile lack courag the left has always been anti israel liberals have always been anti israel libe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQisc7pyZiIp"
      },
      "source": [
        "# Use any dataset and model to calculate Language Model's text level preplexity\n",
        "# You may take help from the following link to have an idea about preplexity.\n",
        "# Use any dataset and model to calculate Language Model's text level preplexity\n",
        "# https://stackoverflow.com/questions/16509685/ngram-model-and-perplexity-in-nltk\n",
        "\n",
        "# (50%)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "T8aeP84tGkcS",
        "outputId": "140e58ec-cc8c-48ea-b77d-0ee4499bee3f"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.model import NgramModel\n",
        "from nltk.probability import LidstoneProbDist\n",
        "\n",
        "\n",
        "# sample text\n",
        "# nltk.download('gutenberg')\n",
        "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
        "\n",
        "# Clean the text\n",
        "sample = sample.lower()\n",
        "sample = re.sub(r'(\\d+\\:\\d+)', '', sample)\n",
        "sample = re.sub(r'[^\\w]', ' ', sample)\n",
        "corpus = ' '.join(sample.split())\n",
        "\n",
        "# Remove rare words from the corpus\n",
        "fdist = nltk.FreqDist(w for w in train)\n",
        "vocabulary = set(map(lambda x: x[0], filter(lambda x: x[1] >= 5, fdist.items())))\n",
        "\n",
        "train = map(lambda x: x if x in vocabulary else \"*unknown*\", train)\n",
        "test = map(lambda x: x if x in vocabulary else \"*unknown*\", test)\n",
        "\n",
        "estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2) \n",
        "lm = NgramModel(5, train, estimator=estimator)\n",
        "\n",
        "print (\"len(corpus) = %s, len(vocabulary) = %s, len(train) = %s, len(test) = %s\" % ( len(corpus), len(vocabulary), len(train), len(test)))\n",
        "print (\"perplexity(test) =\", lm.perplexity(test))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-4a5b0895d6ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgutenberg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNgramModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLidstoneProbDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk.model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf2TI2-_Nasm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}