{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICE_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nm76ryUVg0e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import re\n",
        "\n",
        "data_text = my_text = \"\"\"\n",
        "As I have noted repeatedly, liberalism and leftism have virtually nothing in common. In fact, leftism is the enemy of liberalism — as a handful of liberals such as former New York Times writer Bari Weiss, former Young Turk Dave Rubin, and others have come to recognize. \n",
        "\n",
        "The left has never believed in free speech and has suppressed dissent wherever it has assumed power. Free speech is a pillar of liberalism, and it has always embraced dissent.\n",
        "\n",
        "The left rejects the anti-racist ideal of color-blindness. Colorblind is the liberal racial ideal.\n",
        "\n",
        "The left supports racial segregation — such as all-black dorms and separate black graduations. Liberals have always advocated racial integration.\n",
        "\n",
        "The left has always loathed capitalism. Liberals were always major advocates of capitalism — recognizing that only capitalism has lifted billions of people out of poverty. \n",
        "\n",
        "The left has always been anti-Israel. Liberals have always been fervent supporters of Israel. \n",
        "\n",
        "The left has always held America in contempt. Liberals loved this country. A liberal wrote, “God bless America.” No leftist would write such a song.\n",
        "\n",
        "Leftists want to defund the police. No liberal does.\n",
        "\n",
        "The list of liberal-left differences is as long as the list of left-wing positions.\n",
        "\n",
        "Yet, it is liberals who keep the left in power. Were it not for the liberal vote, the left would have no power.\n",
        "\n",
        "The question is all the more apt given that it is conservatives who protect virtually every liberal value. It is conservatives who seek to preserve free speech, racial integration, love of America, a strong Israel, and capitalism.\n",
        "\n",
        "So why do liberals vote for the left, for the very people who hold liberals and their values in contempt?\n",
        "\n",
        "There are two primary reasons.\n",
        "\n",
        "One is brainwash. Liberals are brainwashed from childhood into believing that the right is their enemy and that pas d’ennemis a gauche (there are “no enemies on the left”). That is why there is no left-wing position, no matter how destructive or vile, that could move a liberal to vote Republican or identify with conservatives.\n",
        "\n",
        "The second reason is fear. Liberals fear they will lose friends and even family if they do not vote Democrat or if they publicly criticize the left. And this is not an irrational fear. \n",
        "\n",
        "America and the West are being destroyed by the left. But this destruction of the universities, the high schools, art and music, journalism, and of freedom itself could not take place were it not for liberals.\n",
        "\n",
        "The fate of America and the West lies largely in the hands of liberals. There are simply not enough leftists to destroy our most revered institutions. They need liberals to serve as fellow travelers to accomplish their ends. \n",
        "\n",
        "Should the American experiment fail — and it may — that profile in lack of courage, the liberal, will have made it possible. \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def text_cleaner(text):\n",
        "    # lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\", \"\", newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    long_words = []\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i) >= 3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "\n",
        "# preprocess the text\n",
        "data_new = text_cleaner(data_text)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVPjY9ZNVg0j",
        "outputId": "a47dcf27-13ef-46f3-f1cb-fc3afac51a10"
      },
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        # select sequence of tokens\n",
        "        seq = text[i - length : i + 1]\n",
        "        # store\n",
        "        sequences.append(seq)\n",
        "    print(\"Total Sequences: %d\" % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "\n",
        "# create sequences\n",
        "sequences = create_seq(data_new)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 2440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7DgLODVg0l"
      },
      "source": [
        "# create a character mapping index\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqUwG4TrVg0m",
        "outputId": "9b45cf8d-f768-427b-d659-d3dc3f393335"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"Train shape:\", X_tr.shape, \"Val shape:\", X_val.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (2196, 30) Val shape: (244, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm4pymHGVg0m",
        "outputId": "a0de89ce-cb1f-4d76-e244-9fe4cf57169f"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 50)            1350      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 150)               90900     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 27)                4077      \n",
            "=================================================================\n",
            "Total params: 96,327\n",
            "Trainable params: 96,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "69/69 - 16s - loss: 2.9866 - acc: 0.1466 - val_loss: 2.8001 - val_acc: 0.1557\n",
            "Epoch 2/100\n",
            "69/69 - 11s - loss: 2.7673 - acc: 0.1931 - val_loss: 2.5918 - val_acc: 0.2951\n",
            "Epoch 3/100\n",
            "69/69 - 11s - loss: 2.4780 - acc: 0.2892 - val_loss: 2.3461 - val_acc: 0.3361\n",
            "Epoch 4/100\n",
            "69/69 - 11s - loss: 2.2742 - acc: 0.3333 - val_loss: 2.2195 - val_acc: 0.3484\n",
            "Epoch 5/100\n",
            "69/69 - 11s - loss: 2.1547 - acc: 0.3752 - val_loss: 2.1310 - val_acc: 0.3852\n",
            "Epoch 6/100\n",
            "69/69 - 11s - loss: 2.0581 - acc: 0.3857 - val_loss: 2.0701 - val_acc: 0.4221\n",
            "Epoch 7/100\n",
            "69/69 - 11s - loss: 1.9767 - acc: 0.4089 - val_loss: 2.0631 - val_acc: 0.4221\n",
            "Epoch 8/100\n",
            "69/69 - 11s - loss: 1.9047 - acc: 0.4235 - val_loss: 2.0099 - val_acc: 0.4098\n",
            "Epoch 9/100\n",
            "69/69 - 11s - loss: 1.8473 - acc: 0.4385 - val_loss: 1.9900 - val_acc: 0.4180\n",
            "Epoch 10/100\n",
            "69/69 - 11s - loss: 1.7771 - acc: 0.4686 - val_loss: 1.9802 - val_acc: 0.4303\n",
            "Epoch 11/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_E4JtfZYhHN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TJrWYndWZ0y"
      },
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict character\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw1h64x4Way2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34pFqhz-XANl"
      },
      "source": [
        "# ICE-8\n",
        "# This ICE is pretty straight forward. Please follow the link below and solve all the coding problems that are explained\n",
        "# https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
        "# Once done with the above part, use any dataset of your choice and build your own language model with the techniques covered in the online tutorial.\n",
        "\n",
        "# (50%)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQisc7pyZiIp"
      },
      "source": [
        "# Use any dataset and model to calculate Language Model's text level preplexity\n",
        "# You may take help from the following link to have an idea about preplexity.\n",
        "# Use any dataset and model to calculate Language Model's text level preplexity\n",
        "# https://stackoverflow.com/questions/16509685/ngram-model-and-perplexity-in-nltk\n",
        "\n",
        "# (50%)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}